#!/usr/bin/env python3
"""
Queztl Protocol Monitor & Analyzer

Real-time protocol monitoring with:
- Packet capture and logging
- Performance metrics extraction
- Anomaly detection
- ML-ready data pipeline
"""

import asyncio
import struct
import json
import time
from datetime import datetime
from collections import defaultdict, deque
from typing import Dict, List, Optional
import sqlite3
import numpy as np

class QueztlMonitor:
    """Real-time protocol monitoring and logging"""
    
    MAGIC = b'QP'
    
    # Message types
    MSG_TYPES = {
        0x01: "COMMAND",
        0x02: "DATA",
        0x03: "STREAM",
        0x04: "ACK",
        0x05: "ERROR",
        0x10: "AUTH",
        0x11: "HEARTBEAT"
    }
    
    def __init__(self, db_path="quetzalcore_monitor.db"):
        self.db_path = db_path
        self.init_database()
        
        # Real-time metrics
        self.message_counts = defaultdict(int)
        self.latency_buffer = deque(maxlen=1000)
        self.error_buffer = deque(maxlen=100)
        self.throughput_window = deque(maxlen=60)  # 60 second window
        
        # Performance tracking
        self.start_time = time.time()
        self.total_bytes_in = 0
        self.total_bytes_out = 0
        self.total_messages = 0
        
    def init_database(self):
        """Initialize SQLite database for protocol logs"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Message log table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS message_log (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp REAL,
                direction TEXT,
                msg_type INTEGER,
                msg_type_name TEXT,
                payload_size INTEGER,
                latency REAL,
                client_id TEXT,
                success INTEGER,
                error_msg TEXT
            )
        """)
        
        # Performance metrics table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS performance_metrics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp REAL,
                throughput REAL,
                avg_latency REAL,
                error_rate REAL,
                cpu_usage REAL,
                memory_usage REAL,
                active_connections INTEGER
            )
        """)
        
        # Protocol anomalies table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS anomalies (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp REAL,
                anomaly_type TEXT,
                severity TEXT,
                description TEXT,
                context TEXT
            )
        """)
        
        # Protocol optimization suggestions
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS optimizations (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp REAL,
                optimization_type TEXT,
                current_value REAL,
                suggested_value REAL,
                expected_improvement REAL,
                confidence REAL,
                applied INTEGER DEFAULT 0
            )
        """)
        
        conn.commit()
        conn.close()
        
    def unpack_message(self, data: bytes) -> Optional[Dict]:
        """Unpack and analyze Queztl Protocol message"""
        try:
            if len(data) < 7:
                return None
                
            magic, msg_type, length = struct.unpack('!2sBL', data[:7])
            
            if magic != self.MAGIC:
                return None
                
            payload = data[7:7+length]
            
            return {
                "msg_type": msg_type,
                "msg_type_name": self.MSG_TYPES.get(msg_type, "UNKNOWN"),
                "payload_size": length,
                "total_size": len(data),
                "overhead": 7,
                "payload": payload
            }
        except Exception as e:
            return None
    
    def log_message(self, direction: str, data: bytes, client_id: str, 
                   latency: Optional[float] = None, success: bool = True, 
                   error_msg: Optional[str] = None):
        """Log a protocol message"""
        msg = self.unpack_message(data)
        if not msg:
            return
            
        timestamp = time.time()
        
        # Update real-time counters
        self.message_counts[msg["msg_type_name"]] += 1
        self.total_messages += 1
        
        if direction == "IN":
            self.total_bytes_in += msg["total_size"]
        else:
            self.total_bytes_out += msg["total_size"]
            
        if latency:
            self.latency_buffer.append(latency)
            
        if not success:
            self.error_buffer.append({
                "timestamp": timestamp,
                "msg_type": msg["msg_type_name"],
                "error": error_msg
            })
        
        # Store in database
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            INSERT INTO message_log 
            (timestamp, direction, msg_type, msg_type_name, payload_size, 
             latency, client_id, success, error_msg)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            timestamp, direction, msg["msg_type"], msg["msg_type_name"],
            msg["payload_size"], latency, client_id, 1 if success else 0, error_msg
        ))
        
        conn.commit()
        conn.close()
        
    def log_performance_snapshot(self, cpu_usage: float, memory_usage: float, 
                                active_connections: int):
        """Log current performance metrics"""
        timestamp = time.time()
        
        # Calculate current metrics
        throughput = len(self.throughput_window)
        avg_latency = np.mean(self.latency_buffer) if self.latency_buffer else 0
        error_rate = len(self.error_buffer) / max(self.total_messages, 1)
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            INSERT INTO performance_metrics
            (timestamp, throughput, avg_latency, error_rate, cpu_usage, 
             memory_usage, active_connections)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (
            timestamp, throughput, avg_latency, error_rate, 
            cpu_usage, memory_usage, active_connections
        ))
        
        conn.commit()
        conn.close()
        
    def detect_anomalies(self) -> List[Dict]:
        """Detect protocol anomalies using statistical methods"""
        anomalies = []
        
        # Check latency spikes
        if len(self.latency_buffer) > 10:
            latencies = np.array(self.latency_buffer)
            mean_latency = np.mean(latencies)
            std_latency = np.std(latencies)
            
            recent_latency = latencies[-10:].mean()
            if recent_latency > mean_latency + 3 * std_latency:
                anomalies.append({
                    "type": "LATENCY_SPIKE",
                    "severity": "HIGH",
                    "description": f"Latency spike detected: {recent_latency:.2f}ms (avg: {mean_latency:.2f}ms)",
                    "context": json.dumps({"recent": recent_latency, "mean": mean_latency, "std": std_latency})
                })
        
        # Check error rate
        recent_errors = sum(1 for e in self.error_buffer if time.time() - e["timestamp"] < 60)
        if recent_errors > 10:
            anomalies.append({
                "type": "HIGH_ERROR_RATE",
                "severity": "CRITICAL",
                "description": f"High error rate: {recent_errors} errors in last 60 seconds",
                "context": json.dumps({"count": recent_errors})
            })
        
        # Check message type imbalance
        total = sum(self.message_counts.values())
        if total > 100:
            for msg_type, count in self.message_counts.items():
                ratio = count / total
                if ratio > 0.7:  # One type dominates
                    anomalies.append({
                        "type": "MESSAGE_IMBALANCE",
                        "severity": "MEDIUM",
                        "description": f"{msg_type} messages dominate traffic ({ratio*100:.1f}%)",
                        "context": json.dumps({"msg_type": msg_type, "ratio": ratio})
                    })
        
        # Store anomalies
        if anomalies:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            for anomaly in anomalies:
                cursor.execute("""
                    INSERT INTO anomalies (timestamp, anomaly_type, severity, description, context)
                    VALUES (?, ?, ?, ?, ?)
                """, (
                    time.time(), anomaly["type"], anomaly["severity"],
                    anomaly["description"], anomaly["context"]
                ))
            
            conn.commit()
            conn.close()
        
        return anomalies
    
    def get_statistics(self) -> Dict:
        """Get current protocol statistics"""
        uptime = time.time() - self.start_time
        
        return {
            "uptime": uptime,
            "total_messages": self.total_messages,
            "total_bytes_in": self.total_bytes_in,
            "total_bytes_out": self.total_bytes_out,
            "messages_per_second": self.total_messages / uptime if uptime > 0 else 0,
            "avg_latency": np.mean(self.latency_buffer) if self.latency_buffer else 0,
            "p95_latency": np.percentile(self.latency_buffer, 95) if len(self.latency_buffer) > 10 else 0,
            "p99_latency": np.percentile(self.latency_buffer, 99) if len(self.latency_buffer) > 10 else 0,
            "error_rate": len(self.error_buffer) / max(self.total_messages, 1),
            "message_counts": dict(self.message_counts),
            "bandwidth_in": self.total_bytes_in / uptime if uptime > 0 else 0,
            "bandwidth_out": self.total_bytes_out / uptime if uptime > 0 else 0
        }
    
    def export_training_data(self, output_file="protocol_training_data.json"):
        """Export data for ML training"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Get message patterns
        cursor.execute("""
            SELECT msg_type, msg_type_name, payload_size, latency, success
            FROM message_log
            ORDER BY timestamp DESC
            LIMIT 10000
        """)
        messages = cursor.fetchall()
        
        # Get performance metrics
        cursor.execute("""
            SELECT throughput, avg_latency, error_rate, cpu_usage, memory_usage
            FROM performance_metrics
            ORDER BY timestamp DESC
            LIMIT 1000
        """)
        performance = cursor.fetchall()
        
        # Get anomalies
        cursor.execute("""
            SELECT anomaly_type, severity, description, context
            FROM anomalies
            ORDER BY timestamp DESC
            LIMIT 100
        """)
        anomalies = cursor.fetchall()
        
        conn.close()
        
        training_data = {
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "total_messages": len(messages),
                "total_performance_snapshots": len(performance),
                "total_anomalies": len(anomalies)
            },
            "messages": [
                {
                    "msg_type": m[0],
                    "msg_type_name": m[1],
                    "payload_size": m[2],
                    "latency": m[3],
                    "success": bool(m[4])
                }
                for m in messages
            ],
            "performance": [
                {
                    "throughput": p[0],
                    "avg_latency": p[1],
                    "error_rate": p[2],
                    "cpu_usage": p[3],
                    "memory_usage": p[4]
                }
                for p in performance
            ],
            "anomalies": [
                {
                    "type": a[0],
                    "severity": a[1],
                    "description": a[2],
                    "context": json.loads(a[3]) if a[3] else {}
                }
                for a in anomalies
            ]
        }
        
        with open(output_file, 'w') as f:
            json.dump(training_data, f, indent=2)
        
        return output_file


class ProtocolAnalyzer:
    """Deep protocol analysis and pattern recognition"""
    
    def __init__(self, monitor: QueztlMonitor):
        self.monitor = monitor
        
    def analyze_message_patterns(self) -> Dict:
        """Analyze message sequencing patterns"""
        conn = sqlite3.connect(self.monitor.db_path)
        cursor = conn.cursor()
        
        # Get message sequences
        cursor.execute("""
            SELECT msg_type_name, latency, payload_size
            FROM message_log
            ORDER BY timestamp DESC
            LIMIT 1000
        """)
        
        messages = cursor.fetchall()
        conn.close()
        
        if not messages:
            return {}
        
        # Analyze patterns
        msg_types = [m[0] for m in messages]
        latencies = [m[1] for m in messages if m[1]]
        sizes = [m[2] for m in messages]
        
        # Common sequences (bigrams)
        sequences = defaultdict(int)
        for i in range(len(msg_types) - 1):
            seq = f"{msg_types[i]} -> {msg_types[i+1]}"
            sequences[seq] += 1
        
        return {
            "total_analyzed": len(messages),
            "unique_types": len(set(msg_types)),
            "avg_latency": np.mean(latencies) if latencies else 0,
            "avg_payload_size": np.mean(sizes),
            "common_sequences": dict(sorted(sequences.items(), key=lambda x: x[1], reverse=True)[:10]),
            "latency_distribution": {
                "min": min(latencies) if latencies else 0,
                "max": max(latencies) if latencies else 0,
                "p50": np.percentile(latencies, 50) if latencies else 0,
                "p95": np.percentile(latencies, 95) if latencies else 0,
                "p99": np.percentile(latencies, 99) if latencies else 0
            },
            "size_distribution": {
                "min": min(sizes),
                "max": max(sizes),
                "avg": np.mean(sizes),
                "median": np.median(sizes)
            }
        }
    
    def analyze_performance_trends(self) -> Dict:
        """Analyze performance trends over time"""
        conn = sqlite3.connect(self.monitor.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT throughput, avg_latency, error_rate, cpu_usage, memory_usage
            FROM performance_metrics
            ORDER BY timestamp DESC
            LIMIT 100
        """)
        
        metrics = cursor.fetchall()
        conn.close()
        
        if not metrics:
            return {}
        
        throughputs = [m[0] for m in metrics]
        latencies = [m[1] for m in metrics]
        error_rates = [m[2] for m in metrics]
        cpu_usages = [m[3] for m in metrics]
        mem_usages = [m[4] for m in metrics]
        
        return {
            "throughput": {
                "current": throughputs[0] if throughputs else 0,
                "avg": np.mean(throughputs),
                "trend": "increasing" if len(throughputs) > 1 and throughputs[0] > throughputs[-1] else "decreasing"
            },
            "latency": {
                "current": latencies[0] if latencies else 0,
                "avg": np.mean(latencies),
                "trend": "increasing" if len(latencies) > 1 and latencies[0] > latencies[-1] else "decreasing"
            },
            "error_rate": {
                "current": error_rates[0] if error_rates else 0,
                "avg": np.mean(error_rates),
                "peak": max(error_rates)
            },
            "resource_usage": {
                "cpu_avg": np.mean(cpu_usages),
                "memory_avg": np.mean(mem_usages)
            }
        }
    
    def suggest_optimizations(self) -> List[Dict]:
        """Suggest protocol optimizations based on analysis"""
        patterns = self.analyze_message_patterns()
        trends = self.analyze_performance_trends()
        
        suggestions = []
        
        # Suggest message batching
        if patterns.get("avg_payload_size", 0) < 100:
            suggestions.append({
                "type": "MESSAGE_BATCHING",
                "current_value": patterns["avg_payload_size"],
                "suggested_value": 500,
                "expected_improvement": 0.3,  # 30% improvement
                "confidence": 0.85,
                "description": "Small payloads detected. Consider batching multiple operations."
            })
        
        # Suggest compression
        if patterns.get("avg_payload_size", 0) > 1000:
            suggestions.append({
                "type": "PAYLOAD_COMPRESSION",
                "current_value": 0,
                "suggested_value": 1,
                "expected_improvement": 0.5,  # 50% size reduction
                "confidence": 0.9,
                "description": "Large payloads detected. Enable compression for >1KB messages."
            })
        
        # Suggest connection pooling
        if trends.get("latency", {}).get("trend") == "increasing":
            suggestions.append({
                "type": "CONNECTION_POOLING",
                "current_value": 1,
                "suggested_value": 5,
                "expected_improvement": 0.4,  # 40% latency reduction
                "confidence": 0.75,
                "description": "Latency increasing. Implement connection pooling."
            })
        
        # Store suggestions
        if suggestions:
            conn = sqlite3.connect(self.monitor.db_path)
            cursor = conn.cursor()
            
            for suggestion in suggestions:
                cursor.execute("""
                    INSERT INTO optimizations
                    (timestamp, optimization_type, current_value, suggested_value,
                     expected_improvement, confidence)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, (
                    time.time(), suggestion["type"], suggestion["current_value"],
                    suggestion["suggested_value"], suggestion["expected_improvement"],
                    suggestion["confidence"]
                ))
            
            conn.commit()
            conn.close()
        
        return suggestions


# CLI for monitoring
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Queztl Protocol Monitor")
    parser.add_argument("--stats", action="store_true", help="Show current statistics")
    parser.add_argument("--analyze", action="store_true", help="Analyze patterns")
    parser.add_argument("--export", action="store_true", help="Export training data")
    parser.add_argument("--suggest", action="store_true", help="Suggest optimizations")
    
    args = parser.parse_args()
    
    monitor = QueztlMonitor()
    analyzer = ProtocolAnalyzer(monitor)
    
    if args.stats:
        stats = monitor.get_statistics()
        print(json.dumps(stats, indent=2))
    
    if args.analyze:
        patterns = analyzer.analyze_message_patterns()
        trends = analyzer.analyze_performance_trends()
        print("Message Patterns:")
        print(json.dumps(patterns, indent=2))
        print("\nPerformance Trends:")
        print(json.dumps(trends, indent=2))
    
    if args.export:
        output = monitor.export_training_data()
        print(f"Training data exported to: {output}")
    
    if args.suggest:
        suggestions = analyzer.suggest_optimizations()
        print("Optimization Suggestions:")
        print(json.dumps(suggestions, indent=2))
