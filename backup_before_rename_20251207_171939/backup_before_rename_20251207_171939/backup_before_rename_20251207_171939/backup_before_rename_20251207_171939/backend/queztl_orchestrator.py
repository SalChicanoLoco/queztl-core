"""
ü¶Ö QUEZTL ORCHESTRATOR - Auto-Scaling VM Manager

Dynamically scales VMs based on workload:
- Small VMs: API requests, light processing
- Medium VMs: Data analysis, model inference  
- Super VMs: Mining MAG processing, 3D generation, heavy ML

Patent-pending: Dynamic resource allocation with predictive scaling
"""

import asyncio
import psutil
import time
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Literal
from enum import Enum
import numpy as np
from backend.native_hypervisor import QueztlHypervisor, VirtualMachine


class VMSize(Enum):
    """VM size tiers"""
    NANO = "nano"          # 1 core, 512MB - health checks, status
    SMALL = "small"        # 2 cores, 2GB - API requests
    MEDIUM = "medium"      # 4 cores, 8GB - data processing
    LARGE = "large"        # 8 cores, 16GB - ML inference
    XLARGE = "xlarge"      # 16 cores, 32GB - heavy computation
    SUPER = "super"        # 32+ cores, 64GB+ - mining analysis, 3D gen


@dataclass
class VMConfig:
    """VM configuration template"""
    size: VMSize
    cpu_cores: int
    memory_mb: int
    gpu_enabled: bool
    max_runtime_seconds: int = 3600
    auto_scale: bool = True
    
    
VM_CONFIGS = {
    VMSize.NANO: VMConfig(VMSize.NANO, 1, 512, False, 60),
    VMSize.SMALL: VMConfig(VMSize.SMALL, 2, 2048, False, 300),
    VMSize.MEDIUM: VMConfig(VMSize.MEDIUM, 4, 8192, True, 1800),
    VMSize.LARGE: VMConfig(VMSize.LARGE, 8, 16384, True, 3600),
    VMSize.XLARGE: VMConfig(VMSize.XLARGE, 16, 32768, True, 7200),
    VMSize.SUPER: VMConfig(VMSize.SUPER, 32, 65536, True, 14400),
}


@dataclass
class WorkloadRequest:
    """Workload request to be executed"""
    workload_id: str
    workload_type: Literal["api", "mining", "3d_gen", "ml_training", "geophysics"]
    priority: int = 5  # 1-10, 10 = highest
    estimated_duration: int = 300  # seconds
    data: dict = field(default_factory=dict)
    required_size: Optional[VMSize] = None  # Auto-detected if None
    

@dataclass
class VMMetrics:
    """VM performance metrics"""
    vm_id: str
    cpu_usage: float
    memory_usage: float
    gpu_usage: float
    uptime: float
    workloads_completed: int
    avg_completion_time: float


class QueztlOrchestrator:
    """
    Auto-scaling VM orchestrator for Queztl Core
    
    Features:
    - Dynamic VM provisioning based on workload
    - Predictive scaling using historical data
    - Resource pooling and reuse
    - Automatic cleanup and optimization
    """
    
    def __init__(self, max_vms: int = 50, enable_super_vms: bool = True):
        self.hypervisor = QueztlHypervisor()
        self.hypervisor.init_gpu_pool(pool_size=16)  # Initialize 16 virtual GPUs
        self.max_vms = max_vms
        self.enable_super_vms = enable_super_vms
        
        # VM pools by size
        self.vm_pools: Dict[VMSize, List[VirtualMachine]] = {
            size: [] for size in VMSize
        }
        
        # Active workloads
        self.active_workloads: Dict[str, WorkloadRequest] = {}
        self.workload_queue: List[WorkloadRequest] = []
        
        # Metrics
        self.vm_metrics: Dict[str, VMMetrics] = {}
        self.workload_history: List[dict] = []
        
        # Auto-scaler state
        self.last_scale_time = time.time()
        self.scale_cooldown = 30  # seconds
        
        print(f"ü¶Ö Queztl Orchestrator initialized")
        print(f"   Max VMs: {max_vms}")
        print(f"   Super VMs: {'ENABLED' if enable_super_vms else 'DISABLED'}")
        
    def detect_workload_size(self, workload: WorkloadRequest) -> VMSize:
        """Auto-detect required VM size based on workload type"""
        
        if workload.required_size:
            return workload.required_size
            
        # Workload type mapping
        size_map = {
            "api": VMSize.SMALL,
            "mining": VMSize.SUPER if self.enable_super_vms else VMSize.XLARGE,
            "3d_gen": VMSize.XLARGE,
            "ml_training": VMSize.SUPER if self.enable_super_vms else VMSize.XLARGE,
            "geophysics": VMSize.LARGE,
        }
        
        base_size = size_map.get(workload.workload_type, VMSize.MEDIUM)
        
        # Adjust based on estimated duration
        if workload.estimated_duration > 3600:  # > 1 hour
            if base_size.value in ["medium", "large"]:
                return VMSize.XLARGE
        
        return base_size
    
    def get_or_create_vm(self, size: VMSize) -> VirtualMachine:
        """Get existing VM from pool or create new one"""
        
        # Try to reuse from pool
        if self.vm_pools[size]:
            vm = self.vm_pools[size].pop(0)
            print(f"‚ôªÔ∏è  Reusing {size.value} VM: {vm.vm_id}")
            return vm
        
        # Check if we can create new VM
        total_vms = sum(len(pool) for pool in self.vm_pools.values())
        total_active = len(self.hypervisor.vms)
        
        if total_vms + total_active >= self.max_vms:
            # Scale down unused VMs
            self._cleanup_idle_vms()
            
            # If still at limit, wait for smallest available
            if total_active >= self.max_vms:
                print(f"‚ö†Ô∏è  VM limit reached ({self.max_vms}), waiting for available VM...")
                # Return smallest available VM (upgrade workload)
                for vm_size in VMSize:
                    if self.vm_pools[vm_size]:
                        return self.vm_pools[vm_size].pop(0)
                raise RuntimeError("No VMs available and cannot create new ones")
        
        # Create new VM
        config = VM_CONFIGS[size]
        vm_id = self.hypervisor.create_vm(
            name=f"{size.value}-vm",
            cpu_cores=config.cpu_cores,
            memory_mb=config.memory_mb,
            gpu_enabled=config.gpu_enabled
        )
        
        vm = self.hypervisor.vms[vm_id]
        print(f"‚ú® Created new {size.value} VM: {vm.vm_id} ({config.cpu_cores} cores, {config.memory_mb}MB)")
        return vm
    
    async def execute_workload(self, workload: WorkloadRequest) -> dict:
        """Execute workload on appropriate VM"""
        
        # Detect required VM size
        required_size = self.detect_workload_size(workload)
        
        print(f"\nüéØ Executing workload: {workload.workload_id}")
        print(f"   Type: {workload.workload_type}")
        print(f"   Required VM: {required_size.value}")
        print(f"   Priority: {workload.priority}")
        
        # Get or create VM
        vm = self.get_or_create_vm(required_size)
        
        # Track workload
        self.active_workloads[workload.workload_id] = workload
        start_time = time.time()
        
        try:
            # Execute workload
            result = await self._run_workload_on_vm(vm, workload)
            
            duration = time.time() - start_time
            
            # Update metrics
            self._update_metrics(vm, duration, success=True)
            
            # Record history
            self.workload_history.append({
                "workload_id": workload.workload_id,
                "type": workload.workload_type,
                "size": required_size.value,
                "duration": duration,
                "success": True,
                "timestamp": time.time()
            })
            
            print(f"‚úÖ Workload completed in {duration:.2f}s")
            
            # Return VM to pool
            self._return_vm_to_pool(vm, required_size)
            
            return result
            
        except Exception as e:
            print(f"‚ùå Workload failed: {e}")
            self._update_metrics(vm, time.time() - start_time, success=False)
            
            # Destroy failed VM
            self.hypervisor.stop_vm(vm.vm_id)
            
            raise
        
        finally:
            del self.active_workloads[workload.workload_id]
    
    async def _run_workload_on_vm(self, vm: VirtualMachine, workload: WorkloadRequest) -> dict:
        """Run workload code on VM"""
        
        # Map workload type to execution function
        workload_funcs = {
            "api": self._execute_api_workload,
            "mining": self._execute_mining_workload,
            "3d_gen": self._execute_3d_gen_workload,
            "ml_training": self._execute_ml_training_workload,
            "geophysics": self._execute_geophysics_workload,
        }
        
        func = workload_funcs.get(workload.workload_type)
        if not func:
            raise ValueError(f"Unknown workload type: {workload.workload_type}")
        
        return await func(vm, workload)
    
    async def _execute_mining_workload(self, vm: VirtualMachine, workload: WorkloadRequest) -> dict:
        """Execute mining MAG survey analysis"""
        
        # Closure to capture workload data
        workload_data = workload.data
        
        def mining_worker():
            """Mining computation in isolated VM"""
            import numpy as np
            
            # Simulate heavy MAG processing
            mag_data = np.array(workload_data.get("mag_readings", []))
            
            # IGRF correction (vectorized)
            igrf_field = np.array([48000, 1500, 52000])  # nT
            corrected = mag_data - igrf_field
            
            # Anomaly detection
            mean = np.mean(corrected)
            std = np.std(corrected)
            anomalies = np.abs(corrected - mean) > 2 * std
            
            # Mineral discrimination (mock)
            signatures = {
                "iron": int(np.sum(corrected > 100)),
                "copper": int(np.sum((corrected > 20) & (corrected < 100))),
                "gold": int(np.sum(np.abs(corrected) < 20))
            }
            
            return {
                "status": "completed",
                "anomalies_detected": int(np.sum(anomalies)),
                "mineral_signatures": signatures,
                "processing_time": 0.0
            }
        
        # Start VM with workload
        self.hypervisor.start_vm(vm.vm_id, mining_worker)
        
        # Wait for completion
        await asyncio.sleep(0.1)  # Small delay for process startup
        vm.process.join(timeout=300)  # 5 min timeout
        
        # Get result from shared memory if available
        result = self.hypervisor.shared_memory.get(f"{vm.vm_id}_result", {
            "status": "completed",
            "vm_id": vm.vm_id
        })
        result["vm_id"] = vm.vm_id
        
        return result
    
    async def _execute_api_workload(self, vm: VirtualMachine, workload: WorkloadRequest) -> dict:
        """Execute lightweight API request"""
        
        workload_data = workload.data
        
        def api_worker():
            return {"status": "completed", "data": workload_data}
        
        self.hypervisor.start_vm(vm.vm_id, api_worker)
        await asyncio.sleep(0.1)
        vm.process.join(timeout=30)
        
        return {"status": "completed", "vm_id": vm.vm_id, "data": workload_data}
    
    async def _execute_3d_gen_workload(self, vm: VirtualMachine, workload: WorkloadRequest) -> dict:
        """Execute 3D model generation"""
        
        def gen3d_worker():
            import numpy as np
            # Simulate 3D generation
            vertices = np.random.rand(1000, 3) * 100
            return {"vertices": vertices.tolist(), "status": "completed"}
        
        self.hypervisor.start_vm(vm.vm_id, gen3d_worker)
        await asyncio.sleep(0.1)
        vm.process.join(timeout=180)
        
        return {"status": "completed", "vm_id": vm.vm_id, "model": "3d_generated"}
    
    async def _execute_ml_training_workload(self, vm: VirtualMachine, workload: WorkloadRequest) -> dict:
        """Execute ML model training"""
        
        workload_data = workload.data
        
        def ml_worker():
            import numpy as np
            # Simulate training
            epochs = workload_data.get("epochs", 10)
            loss = 1.0
            for i in range(epochs):
                loss *= 0.9
            return {"final_loss": loss, "epochs": epochs, "status": "completed"}
        
        self.hypervisor.start_vm(vm.vm_id, ml_worker)
        await asyncio.sleep(0.1)
        vm.process.join(timeout=600)
        
        return {"status": "completed", "vm_id": vm.vm_id, "training": "complete"}
    
    async def _execute_geophysics_workload(self, vm: VirtualMachine, workload: WorkloadRequest) -> dict:
        """Execute geophysics computation"""
        
        def geo_worker(data):
            return {"status": "completed", "computation": "geophysics"}
        
        return self.hypervisor.start_vm(vm.vm_id, geo_worker, workload.data)
    
    def _return_vm_to_pool(self, vm: VirtualMachine, size: VMSize):
        """Return VM to pool for reuse"""
        
        # Get VM stats
        stats = self.hypervisor.get_vm_stats(vm.vm_id)
        
        # Check if VM is healthy
        if stats["status"] == "running":
            self.vm_pools[size].append(vm)
            print(f"‚ôªÔ∏è  Returned {size.value} VM to pool: {vm.vm_id}")
        else:
            print(f"üóëÔ∏è  Destroying unhealthy VM: {vm.vm_id}")
            self.hypervisor.stop_vm(vm.vm_id)
    
    def _update_metrics(self, vm: VirtualMachine, duration: float, success: bool):
        """Update VM performance metrics"""
        
        if vm.vm_id not in self.vm_metrics:
            stats = self.hypervisor.get_vm_stats(vm.vm_id)
            self.vm_metrics[vm.vm_id] = VMMetrics(
                vm_id=vm.vm_id,
                cpu_usage=stats.get("cpu_percent", 0),
                memory_usage=stats.get("memory_mb", 0),
                gpu_usage=0.0,
                uptime=stats.get("uptime", 0),
                workloads_completed=0,
                avg_completion_time=0.0
            )
        
        metrics = self.vm_metrics[vm.vm_id]
        if success:
            metrics.workloads_completed += 1
            # Running average
            n = metrics.workloads_completed
            metrics.avg_completion_time = (metrics.avg_completion_time * (n-1) + duration) / n
    
    def _cleanup_idle_vms(self):
        """Clean up idle VMs to free resources"""
        
        print("üßπ Cleaning up idle VMs...")
        cleaned = 0
        
        for size, pool in self.vm_pools.items():
            if len(pool) > 2:  # Keep max 2 VMs per size in pool
                excess = pool[2:]
                for vm in excess:
                    self.hypervisor.stop_vm(vm.vm_id)
                    cleaned += 1
                self.vm_pools[size] = pool[:2]
        
        if cleaned > 0:
            print(f"   Destroyed {cleaned} idle VMs")
    
    def get_cluster_status(self) -> dict:
        """Get overall cluster status"""
        
        total_vms = len(self.hypervisor.vms)
        pooled_vms = sum(len(pool) for pool in self.vm_pools.values())
        
        # System resources
        cpu_total = psutil.cpu_count()
        cpu_used = psutil.cpu_percent()
        mem = psutil.virtual_memory()
        
        # VM breakdown
        vm_breakdown = {
            size.value: len(pool) for size, pool in self.vm_pools.items()
        }
        
        return {
            "status": "operational",
            "timestamp": time.time(),
            "vms": {
                "active": total_vms,
                "pooled": pooled_vms,
                "total": total_vms + pooled_vms,
                "max": self.max_vms,
                "breakdown": vm_breakdown
            },
            "resources": {
                "cpu_cores": cpu_total,
                "cpu_usage_percent": cpu_used,
                "memory_total_mb": mem.total // (1024**2),
                "memory_used_mb": mem.used // (1024**2),
                "memory_available_mb": mem.available // (1024**2)
            },
            "workloads": {
                "active": len(self.active_workloads),
                "queued": len(self.workload_queue),
                "completed": len(self.workload_history)
            },
            "super_vms_enabled": self.enable_super_vms
        }
    
    async def auto_scale(self):
        """Auto-scale VMs based on workload predictions"""
        
        # Check cooldown
        if time.time() - self.last_scale_time < self.scale_cooldown:
            return
        
        # Analyze queue
        if len(self.workload_queue) > 5:
            print("üìà Scaling up: High queue depth")
            # Pre-provision VMs
            for _ in range(min(3, self.max_vms - len(self.hypervisor.vms))):
                size = VMSize.MEDIUM
                vm_id = self.hypervisor.create_vm(
                    name=f"autoscale-{size.value}",
                    cpu_cores=VM_CONFIGS[size].cpu_cores,
                    memory_mb=VM_CONFIGS[size].memory_mb,
                    gpu_enabled=True
                )
                vm = self.hypervisor.vms[vm_id]
                self.vm_pools[size].append(vm)
        
        elif len(self.active_workloads) == 0 and sum(len(p) for p in self.vm_pools.values()) > 5:
            print("üìâ Scaling down: Low utilization")
            self._cleanup_idle_vms()
        
        self.last_scale_time = time.time()


# Example usage
async def main():
    orchestrator = QueztlOrchestrator(max_vms=20, enable_super_vms=True)
    
    # Test different workload types
    workloads = [
        WorkloadRequest("w1", "api", priority=3, estimated_duration=10),
        WorkloadRequest("w2", "mining", priority=10, estimated_duration=600, 
                       data={"mag_readings": list(range(1000))}),
        WorkloadRequest("w3", "3d_gen", priority=7, estimated_duration=300),
        WorkloadRequest("w4", "mining", priority=10, estimated_duration=900,
                       data={"mag_readings": list(range(2000))}),
    ]
    
    # Execute workloads
    for workload in workloads:
        try:
            result = await orchestrator.execute_workload(workload)
            print(f"Result: {result}\n")
        except Exception as e:
            print(f"Error: {e}\n")
    
    # Show cluster status
    status = orchestrator.get_cluster_status()
    print("\n" + "="*60)
    print("ü¶Ö QUEZTL CLUSTER STATUS")
    print("="*60)
    print(f"VMs Active: {status['vms']['active']}")
    print(f"VMs Pooled: {status['vms']['pooled']}")
    print(f"VMs Total: {status['vms']['total']} / {status['vms']['max']}")
    print(f"CPU Usage: {status['resources']['cpu_usage_percent']:.1f}%")
    print(f"Memory: {status['resources']['memory_used_mb']}MB / {status['resources']['memory_total_mb']}MB")
    print(f"Workloads Completed: {status['workloads']['completed']}")
    print(f"Super VMs: {'‚úÖ ENABLED' if status['super_vms_enabled'] else '‚ùå DISABLED'}")


if __name__ == "__main__":
    asyncio.run(main())
