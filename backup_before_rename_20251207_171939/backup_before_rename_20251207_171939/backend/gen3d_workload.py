"""
ðŸŽ¨ GEN3D WORKLOAD - AI 3D GENERATION FOR HIVE
Distributed 3D model generation with on-demand agent scaling

Features:
- Text-to-3D distributed across workers
- Image-to-3D with GPU acceleration
- Auto-spawn workers on demand
- Load balancing for parallel generation
- Shap-E model distribution

================================================================================
Copyright (c) 2025 Queztl-Core Project
================================================================================
"""

import asyncio
import time
import hashlib
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from enum import Enum
import logging

logger = logging.getLogger(__name__)


class Gen3DTaskType(Enum):
    """Types of 3D generation tasks"""
    TEXT_TO_3D = "text_to_3d"
    IMAGE_TO_3D = "image_to_3d"
    VIDEO_TO_3D = "video_to_3d"
    BATCH_GENERATION = "batch_generation"


@dataclass
class Gen3DTask:
    """3D generation task for distributed execution"""
    task_id: str
    task_type: Gen3DTaskType
    prompt: Optional[str] = None
    image_data: Optional[bytes] = None
    video_data: Optional[bytes] = None
    
    # Generation parameters
    style: str = "realistic"
    detail_level: str = "medium"
    model: str = "shap-e"
    
    # Distributed execution
    assigned_node: Optional[str] = None
    status: str = "pending"  # pending, running, completed, failed
    progress: float = 0.0
    
    # Timing
    created_at: float = 0.0
    started_at: float = 0.0
    completed_at: float = 0.0
    
    # Results
    result_data: Optional[Dict[str, Any]] = None
    error_message: Optional[str] = None
    
    # Resource requirements
    requires_gpu: bool = True
    estimated_duration: float = 30.0  # seconds
    memory_gb: float = 8.0


class Gen3DWorkloadManager:
    """
    Manages Gen3D workloads across distributed Hive network
    Spawns workers on-demand for 3D generation tasks
    """
    
    def __init__(self, hive_scheduler, hive_autoscaler):
        self.scheduler = hive_scheduler
        self.autoscaler = hive_autoscaler
        self.tasks: Dict[str, Gen3DTask] = {}
        self.task_queue = asyncio.Queue()
        self.active_workers = 0
        self.max_workers = 10
        
        # Performance tracking
        self.total_tasks = 0
        self.completed_tasks = 0
        self.failed_tasks = 0
        self.avg_generation_time = 0.0
        
        # Auto-scaling thresholds
        self.scale_up_queue_depth = 5
        self.scale_down_idle_time = 300  # 5 minutes
        
        logger.info("Gen3D Workload Manager initialized")
    
    async def submit_task(self, task: Gen3DTask) -> str:
        """
        Submit a new 3D generation task
        Automatically scales workers if needed
        """
        # Generate task ID
        if not task.task_id:
            task_data = f"{task.task_type}-{task.prompt}-{time.time()}"
            task.task_id = hashlib.sha256(task_data.encode()).hexdigest()[:16]
        
        task.created_at = time.time()
        task.status = "pending"
        
        # Store task
        self.tasks[task.task_id] = task
        self.total_tasks += 1
        
        # Add to queue
        await self.task_queue.put(task)
        
        # Check if we need to scale up
        queue_depth = self.task_queue.qsize()
        if queue_depth > self.scale_up_queue_depth:
            await self._scale_up_workers()
        
        logger.info(f"Task {task.task_id} submitted (queue depth: {queue_depth})")
        return task.task_id
    
    async def _scale_up_workers(self):
        """Spawn new workers on-demand for Gen3D tasks"""
        current_workers = self.active_workers
        target_workers = min(
            current_workers + 2,  # Add 2 workers at a time
            self.max_workers
        )
        
        if current_workers >= self.max_workers:
            logger.warning("Max workers reached, cannot scale up")
            return
        
        workers_to_spawn = target_workers - current_workers
        logger.info(f"ðŸš€ Scaling up: spawning {workers_to_spawn} Gen3D workers")
        
        # Create worker nodes via autoscaler
        for i in range(workers_to_spawn):
            await self.autoscaler.spawn_gen3d_worker()
            self.active_workers += 1
        
        logger.info(f"âœ… Scaled to {self.active_workers} Gen3D workers")
    
    async def _scale_down_workers(self):
        """Remove idle workers to save resources"""
        # Check if workers are idle
        idle_workers = self.active_workers - (self.task_queue.qsize() // 2)
        if idle_workers > 1:  # Keep at least 1 worker
            workers_to_remove = idle_workers - 1
            logger.info(f"ðŸ“‰ Scaling down: removing {workers_to_remove} idle workers")
            
            for i in range(workers_to_remove):
                await self.autoscaler.terminate_idle_gen3d_worker()
                self.active_workers -= 1
            
            logger.info(f"âœ… Scaled down to {self.active_workers} Gen3D workers")
    
    async def get_task_status(self, task_id: str) -> Optional[Dict[str, Any]]:
        """Get status of a 3D generation task"""
        if task_id not in self.tasks:
            return None
        
        task = self.tasks[task_id]
        return {
            "task_id": task.task_id,
            "status": task.status,
            "progress": task.progress,
            "assigned_node": task.assigned_node,
            "created_at": task.created_at,
            "started_at": task.started_at,
            "completed_at": task.completed_at,
            "result": task.result_data,
            "error": task.error_message
        }
    
    async def get_task_result(self, task_id: str) -> Optional[Dict[str, Any]]:
        """Get result of completed task"""
        task = self.tasks.get(task_id)
        if not task or task.status != "completed":
            return None
        
        return task.result_data
    
    async def cancel_task(self, task_id: str) -> bool:
        """Cancel a pending or running task"""
        task = self.tasks.get(task_id)
        if not task:
            return False
        
        if task.status in ["pending", "running"]:
            task.status = "cancelled"
            logger.info(f"Task {task_id} cancelled")
            return True
        
        return False
    
    def get_stats(self) -> Dict[str, Any]:
        """Get Gen3D workload statistics"""
        pending = sum(1 for t in self.tasks.values() if t.status == "pending")
        running = sum(1 for t in self.tasks.values() if t.status == "running")
        
        return {
            "total_tasks": self.total_tasks,
            "completed_tasks": self.completed_tasks,
            "failed_tasks": self.failed_tasks,
            "pending_tasks": pending,
            "running_tasks": running,
            "active_workers": self.active_workers,
            "queue_depth": self.task_queue.qsize(),
            "avg_generation_time": self.avg_generation_time,
            "success_rate": self.completed_tasks / max(1, self.total_tasks)
        }
    
    async def process_tasks(self):
        """Background task processor - runs continuously"""
        logger.info("Gen3D task processor started")
        
        while True:
            try:
                # Get next task from queue
                task = await asyncio.wait_for(
                    self.task_queue.get(),
                    timeout=60.0  # Check every minute
                )
                
                # Find available node for this task
                node = await self._find_best_node(task)
                
                if node:
                    # Execute task on node
                    await self._execute_task_on_node(task, node)
                else:
                    # No nodes available, re-queue
                    logger.warning(f"No available nodes for task {task.task_id}, re-queuing")
                    await self.task_queue.put(task)
                    await asyncio.sleep(5)
                
            except asyncio.TimeoutError:
                # No tasks in queue, check if we should scale down
                if self.task_queue.empty():
                    await self._scale_down_workers()
            
            except Exception as e:
                logger.error(f"Error processing tasks: {e}")
                await asyncio.sleep(5)
    
    async def _find_best_node(self, task: Gen3DTask):
        """Find best node for task execution"""
        # Get available nodes with required capabilities
        required_caps = []
        if task.requires_gpu:
            required_caps.append("ai_inference")
        
        nodes = await self.scheduler.registry.get_available_nodes(
            required_capabilities=required_caps,
            min_ram_gb=task.memory_gb
        )
        
        if nodes:
            return nodes[0]  # Return least loaded node
        return None
    
    async def _execute_task_on_node(self, task: Gen3DTask, node):
        """Execute 3D generation task on specific node"""
        task.status = "running"
        task.started_at = time.time()
        task.assigned_node = node.node_id
        
        logger.info(f"Executing task {task.task_id} on node {node.node_id}")
        
        try:
            # Call node's Gen3D API endpoint
            result = await self._call_node_gen3d_api(node, task)
            
            task.status = "completed"
            task.completed_at = time.time()
            task.result_data = result
            task.progress = 1.0
            
            self.completed_tasks += 1
            
            # Update average generation time
            duration = task.completed_at - task.started_at
            self.avg_generation_time = (
                (self.avg_generation_time * (self.completed_tasks - 1) + duration)
                / self.completed_tasks
            )
            
            logger.info(f"âœ… Task {task.task_id} completed in {duration:.2f}s")
            
        except Exception as e:
            task.status = "failed"
            task.completed_at = time.time()
            task.error_message = str(e)
            self.failed_tasks += 1
            
            logger.error(f"âŒ Task {task.task_id} failed: {e}")
    
    async def _call_node_gen3d_api(self, node, task: Gen3DTask) -> Dict[str, Any]:
        """Call Gen3D API on specific node"""
        import aiohttp
        
        # Build API URL
        api_url = f"http://{node.ip_address}:8001/api/{task.task_type.value}"
        
        # Build request parameters
        params = {
            "prompt": task.prompt,
            "style": task.style,
            "detail_level": task.detail_level,
            "model": task.model
        }
        
        # Make API call
        async with aiohttp.ClientSession() as session:
            async with session.get(api_url, params=params, timeout=300) as response:
                if response.status == 200:
                    return await response.json()
                else:
                    raise Exception(f"API call failed with status {response.status}")


# ============================================================================
# AUTOSCALER EXTENSION FOR GEN3D
# ============================================================================

class Gen3DAutoScaler:
    """Auto-scaler specifically for Gen3D workers"""
    
    def __init__(self, base_autoscaler):
        self.base = base_autoscaler
        self.gen3d_node_template = None
    
    async def spawn_gen3d_worker(self):
        """Spawn a new Gen3D worker node"""
        logger.info("Spawning Gen3D worker via Docker...")
        
        # Use Docker to spawn Gen3D container as worker
        import subprocess
        
        try:
            # Run gen3d-backend as a worker node
            cmd = [
                "docker", "run", "-d",
                "--name", f"gen3d-worker-{int(time.time())}",
                "--network", "hive_default",  # Connect to Hive network
                "-p", "8001",  # Dynamic port
                "gen3d-app-gen3d-backend"
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                container_id = result.stdout.strip()
                logger.info(f"âœ… Spawned Gen3D worker: {container_id[:12]}")
                return container_id
            else:
                logger.error(f"Failed to spawn worker: {result.stderr}")
                return None
        
        except Exception as e:
            logger.error(f"Error spawning Gen3D worker: {e}")
            return None
    
    async def terminate_idle_gen3d_worker(self):
        """Terminate an idle Gen3D worker"""
        logger.info("Terminating idle Gen3D worker...")
        
        import subprocess
        
        try:
            # Find idle gen3d-worker containers
            cmd = ["docker", "ps", "--filter", "name=gen3d-worker", "--format", "{{.ID}}"]
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0 and result.stdout.strip():
                containers = result.stdout.strip().split('\n')
                if containers:
                    container_id = containers[0]
                    
                    # Stop and remove
                    subprocess.run(["docker", "stop", container_id], check=True)
                    subprocess.run(["docker", "rm", container_id], check=True)
                    
                    logger.info(f"âœ… Terminated Gen3D worker: {container_id[:12]}")
                    return True
            
            return False
        
        except Exception as e:
            logger.error(f"Error terminating worker: {e}")
            return False
