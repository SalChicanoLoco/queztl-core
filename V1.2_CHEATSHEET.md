# ‚ö° QUEZTL v1.2 - QUICK REFERENCE CHEAT SHEET

## üöÄ Start System
```bash
./start.sh                    # Start all services
./test-v1.2-distributed.sh   # Test v1.2 features
```

## üìä Check Status
```bash
# Network status (nodes, tasks, resources)
curl http://localhost:8000/api/v1.2/network/status | jq

# Auto-scaler status (scaling metrics)
curl http://localhost:8000/api/v1.2/autoscaler/status | jq

# Health check
curl http://localhost:8000/api/health
```

## üìà Submit Workloads
```bash
# LLM Inference (tokens/sec)
curl -X POST http://localhost:8000/api/v1.2/workload/submit \
  -H "Content-Type: application/json" \
  -d '{"workload_type":"llm_inference","payload":{"model_size":"7B"},"priority":8}'

# Image Processing (fps)
curl -X POST http://localhost:8000/api/v1.2/workload/submit \
  -H "Content-Type: application/json" \
  -d '{"workload_type":"image_filter","payload":{"resolution":"4K"},"priority":7}'

# Video Encoding (fps)
curl -X POST http://localhost:8000/api/v1.2/workload/submit \
  -H "Content-Type: application/json" \
  -d '{"workload_type":"video_encode_h264","payload":{"resolution":"4K"},"priority":9}'

# Crypto Mining (MH/s)
curl -X POST http://localhost:8000/api/v1.2/workload/submit \
  -H "Content-Type: application/json" \
  -d '{"workload_type":"crypto_mining","payload":{"algorithm":"SHA256"},"priority":5}'

# Database Query (queries/sec)
curl -X POST http://localhost:8000/api/v1.2/workload/submit \
  -H "Content-Type: application/json" \
  -d '{"workload_type":"sql_query","payload":{"type":"aggregation"},"priority":6}'
```

## üéØ Real-World Benchmarks
```bash
# Run complete benchmark suite (~60 seconds)
curl http://localhost:8000/api/v1.2/benchmarks/realworld | jq '.benchmarks[] | {name, score, unit}'

# Pretty print results
curl -s http://localhost:8000/api/v1.2/benchmarks/realworld | python3 -c "
import sys, json
data = json.load(sys.stdin)
for b in data['benchmarks']:
    print(f\"{b['name']}: {b['score']:.2f} {b['unit']}\")
"
```

## ‚öñÔ∏è Manual Scaling
```bash
# Scale UP (add 5 nodes)
curl -X POST http://localhost:8000/api/v1.2/scale/manual \
  -H "Content-Type: application/json" \
  -d '{"action":"up","count":5}'

# Scale DOWN (remove 3 nodes)
curl -X POST http://localhost:8000/api/v1.2/scale/manual \
  -H "Content-Type: application/json" \
  -d '{"action":"down","count":3}'

# Check current node count
curl -s http://localhost:8000/api/v1.2/network/status | jq '.registry.total_nodes'
```

## üîß Add Worker Nodes

### Docker (Local)
```bash
docker run -d --name queztl-worker-1 \
  -e COORDINATOR_URL=http://host.docker.internal:8000 \
  -e NODE_TYPE=worker_cpu \
  queztl-worker:latest
```

### Python Script (Any Machine)
```python
# On worker machine
from distributed_network import WorkerNode
import asyncio

worker = WorkerNode(
    coordinator_url='http://master-ip:8000',
    port=8001
)

asyncio.run(worker.start())
```

### VM/Cloud Instance
```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Set environment
export QUEZTL_MODE=worker
export QUEZTL_COORDINATOR_URL=http://master-ip:8000
export QUEZTL_PORT=8001

# 3. Start worker
python -c "from distributed_network import WorkerNode; import asyncio; asyncio.run(WorkerNode('$QUEZTL_COORDINATOR_URL', $QUEZTL_PORT).start())"
```

## üì° Monitor Real-Time
```bash
# Watch node count
watch -n 1 'curl -s http://localhost:8000/api/v1.2/network/status | jq ".registry.total_nodes"'

# Watch task queue
watch -n 1 'curl -s http://localhost:8000/api/v1.2/network/status | jq ".scheduler"'

# Watch auto-scaler
watch -n 5 'curl -s http://localhost:8000/api/v1.2/autoscaler/status | jq'

# Watch specific task
watch -n 1 'curl -s http://localhost:8000/api/v1.2/workload/TASK_ID/status | jq'
```

## üåç Geographic Distribution

Nodes automatically deploy to optimal regions based on workload sources:
- üá∫üá∏ US East (Virginia)
- üá∫üá∏ US West (California)  
- üá™üá∫ EU West (Ireland)
- üáØüáµ Asia Pacific (Tokyo)

## üí∞ Cost Estimates (AWS Spot Instances)

| Instance Type | vCPU | RAM | GPU | Cost/Hour | 10 Nodes/Hour |
|---------------|------|-----|-----|-----------|---------------|
| t3.micro      | 2    | 1GB | -   | $0.0031   | $0.031        |
| t3.medium     | 2    | 4GB | -   | $0.0104   | $0.104        |
| c5.2xlarge    | 8    | 16GB| -   | $0.085    | $0.850        |
| g4dn.xlarge   | 4    | 16GB| T4  | $0.156    | $1.560        |

**Example**: 50 t3.medium nodes for 1 hour = **$0.52**

## üéØ Scaling Policies

### Reactive (Default)
Responds to current load immediately
```python
policy=ScalingPolicy.REACTIVE
target=ScalingTarget(
    scale_up_threshold=0.85,    # Scale up at 85% CPU
    scale_down_threshold=0.30,  # Scale down at 30% CPU
)
```

### Predictive (ML-Based)
Forecasts demand spikes before they happen
```python
policy=ScalingPolicy.PREDICTIVE  # Uses historical patterns
```

### Cost-Optimized
Minimizes cloud costs with spot instances
```python
policy=ScalingPolicy.COST_OPTIMIZED  # Max spot usage
```

## üîç Troubleshooting

| Problem | Solution |
|---------|----------|
| Nodes not registering | Check network connectivity, firewall rules |
| Auto-scaler not scaling | Check if in cooldown period or at min/max |
| High task failure rate | Check node resources (CPU, memory limits) |
| Slow performance | Increase `max_nodes`, check network latency |
| Cloud instances not launching | Verify cloud credentials, region availability |

## üìö Key Files

| File | Purpose |
|------|---------|
| `backend/distributed_network.py` | Core networking, node registry, scheduler |
| `backend/autoscaler.py` | Dynamic scaling engine |
| `backend/real_world_benchmarks.py` | Industry-standard benchmarks |
| `backend/main.py` | FastAPI server with v1.2 endpoints |
| `V1.2_DISTRIBUTED_SCALING.md` | Complete documentation |
| `V1.2_SUMMARY.md` | Quick overview |
| `test-v1.2-distributed.sh` | Automated testing |

## üéÆ Common Use Cases

### AI/ML Training
```bash
# Train model across 50 GPUs
curl -X POST http://localhost:8000/api/v1.2/workload/submit \
  -d '{"workload_type":"neural_training","payload":{"model":"llama-7b"},"priority":10}'
```

### Video Processing Pipeline
```bash
# Encode 1000 videos in parallel
for i in {1..1000}; do
  curl -X POST http://localhost:8000/api/v1.2/workload/submit \
    -d "{\"workload_type\":\"video_encode_h264\",\"payload\":{\"video\":\"video$i.mp4\"}}"
done
```

### Scientific Simulation
```bash
# Monte Carlo with 10M iterations
curl -X POST http://localhost:8000/api/v1.2/workload/submit \
  -d '{"workload_type":"monte_carlo","payload":{"iterations":10000000},"priority":9}'
```

### Real-time Analytics
```bash
# Stream processing
curl -X POST http://localhost:8000/api/v1.2/workload/submit \
  -d '{"workload_type":"stream_processing","payload":{"source":"kafka://..."},"priority":10}'
```

## üöÄ Performance Targets

| Metric | Single Node | 10 Nodes | 100 Nodes |
|--------|-------------|----------|-----------|
| Tasks/hour | ~1,000 | ~10,000 | ~100,000 |
| Latency | 5-10s | 1-2s | <1s |
| Throughput | 1x | 10x | 100x |
| Cost (AWS spot) | $0.01/hr | $0.10/hr | $1.00/hr |

## üìû API Endpoint Summary

```
GET  /api/health                          # System health
GET  /api/v1.2/network/status            # Network overview
GET  /api/v1.2/autoscaler/status         # Scaling metrics
POST /api/v1.2/workload/submit           # Submit distributed task
GET  /api/v1.2/workload/{id}/status      # Task status
POST /api/v1.2/nodes/register            # Register worker
POST /api/v1.2/nodes/{id}/heartbeat      # Worker heartbeat
GET  /api/v1.2/benchmarks/realworld      # Run benchmarks
POST /api/v1.2/scale/manual              # Manual scaling
```

---

**Master these commands and you control a massively scalable distributed compute network!** üöÄ
