"""
GIS-Geophysics Adaptive Improvement Engine
Continuously learns from new data and feedback to improve accuracy and efficiency
"""
import numpy as np
from typing import Dict, List, Tuple, Optional, Any, Callable
from dataclasses import dataclass, field
from enum import Enum
import logging
from datetime import datetime, timedelta
import json
from collections import defaultdict

logger = logging.getLogger(__name__)


class ImprovementStrategy(Enum):
    """Strategies for model improvement"""
    FEEDBACK_LOOP = "feedback_loop"  # Learn from user corrections
    ERROR_ANALYSIS = "error_analysis"  # Analyze and fix systematic errors
    DATA_AUGMENTATION = "data_augmentation"  # Generate synthetic training data
    ENSEMBLE_BOOSTING = "ensemble_boosting"  # Combine models for better performance
    TRANSFER_LEARNING = "transfer_learning"  # Learn from similar domains


@dataclass
class Feedback:
    """User feedback on predictions"""
    prediction_id: str
    predicted_value: Any
    ground_truth: Any
    confidence: float
    feedback_type: str  # "correct", "incorrect", "partial", "uncertain"
    timestamp: str = field(default_factory=lambda: datetime.utcnow().isoformat())
    user_notes: Optional[str] = None


@dataclass
class ErrorMetric:
    """Tracked error metric"""
    metric_name: str
    value: float
    threshold: float
    status: str  # "good", "warning", "critical"
    timestamp: str = field(default_factory=lambda: datetime.utcnow().isoformat())


@dataclass
class ImprovementAction:
    """Action taken to improve model"""
    strategy: ImprovementStrategy
    description: str
    expected_improvement: float  # Expected % improvement
    actual_improvement: Optional[float] = None
    status: str = "planned"  # "planned", "in_progress", "completed", "failed"
    timestamp: str = field(default_factory=lambda: datetime.utcnow().isoformat())
    details: Dict[str, Any] = field(default_factory=dict)


class AdaptiveImprovementEngine:
    """
    Continuously improves GIS-Geophysics models through:
    - Feedback collection and analysis
    - Error tracking and diagnosis
    - Automated retraining
    - Performance monitoring
    """
    
    def __init__(self):
        self.feedback_log: List[Feedback] = []
        self.error_metrics: Dict[str, List[ErrorMetric]] = defaultdict(list)
        self.improvement_actions: List[ImprovementAction] = []
        self.performance_baseline: Dict[str, float] = {}
        self.model_versions: List[Dict[str, Any]] = []
        self.logger = logging.getLogger(__name__)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # FEEDBACK COLLECTION & ANALYSIS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def collect_feedback(self, prediction_id: str, predicted_value: Any,
                        ground_truth: Any, confidence: float,
                        user_notes: Optional[str] = None) -> Feedback:
        """
        Collect user feedback on a prediction
        """
        # Determine feedback type
        if isinstance(predicted_value, (int, float)) and isinstance(ground_truth, (int, float)):
            error_percent = abs(predicted_value - ground_truth) / (abs(ground_truth) + 1e-6) * 100
            if error_percent < 5:
                feedback_type = "correct"
            elif error_percent < 20:
                feedback_type = "partial"
            else:
                feedback_type = "incorrect"
        else:
            feedback_type = "correct" if predicted_value == ground_truth else "incorrect"
        
        feedback = Feedback(
            prediction_id=prediction_id,
            predicted_value=predicted_value,
            ground_truth=ground_truth,
            confidence=confidence,
            feedback_type=feedback_type,
            user_notes=user_notes
        )
        
        self.feedback_log.append(feedback)
        self.logger.info(f"âœ… Feedback collected: {feedback_type} (confidence: {confidence:.2f})")
        
        return feedback
    
    def analyze_feedback(self, lookback_hours: int = 24) -> Dict[str, Any]:
        """
        Analyze collected feedback over time period
        """
        cutoff_time = datetime.utcnow() - timedelta(hours=lookback_hours)
        cutoff_iso = cutoff_time.isoformat()
        
        relevant_feedback = [
            f for f in self.feedback_log
            if f.timestamp >= cutoff_iso
        ]
        
        if not relevant_feedback:
            return {
                "period_hours": lookback_hours,
                "feedback_count": 0,
                "message": "No feedback collected in period"
            }
        
        # Calculate statistics
        feedback_types = defaultdict(int)
        confidence_scores = []
        error_magnitudes = []
        
        for fb in relevant_feedback:
            feedback_types[fb.feedback_type] += 1
            confidence_scores.append(fb.confidence)
            
            if isinstance(fb.predicted_value, (int, float)) and isinstance(fb.ground_truth, (int, float)):
                error = abs(fb.predicted_value - fb.ground_truth)
                error_magnitudes.append(error)
        
        analysis = {
            "period_hours": lookback_hours,
            "feedback_count": len(relevant_feedback),
            "feedback_distribution": dict(feedback_types),
            "accuracy_rate": f"{(feedback_types.get('correct', 0) / len(relevant_feedback) * 100):.1f}%",
            "average_confidence": float(np.mean(confidence_scores)),
            "confidence_std": float(np.std(confidence_scores)),
            "improvement_needed": feedback_types.get("incorrect", 0) > feedback_types.get("correct", 0),
            "high_priority_fixes": self._identify_problem_areas(relevant_feedback)
        }
        
        if error_magnitudes:
            analysis["error_statistics"] = {
                "mean": float(np.mean(error_magnitudes)),
                "median": float(np.median(error_magnitudes)),
                "std": float(np.std(error_magnitudes)),
                "max": float(np.max(error_magnitudes))
            }
        
        return analysis
    
    def _identify_problem_areas(self, feedback: List[Feedback]) -> List[Dict[str, Any]]:
        """Identify systematic errors"""
        error_categories = defaultdict(list)
        
        for fb in feedback:
            if fb.feedback_type == "incorrect":
                # Group errors
                category_key = f"{type(fb.predicted_value).__name__}_{fb.feedback_type}"
                error_categories[category_key].append(fb)
        
        # Identify problem areas
        problem_areas = []
        for category, errors in error_categories.items():
            if len(errors) >= 3:  # At least 3 similar errors
                problem_areas.append({
                    "category": category,
                    "error_count": len(errors),
                    "severity": "high" if len(errors) >= 5 else "medium",
                    "priority": 1 if len(errors) >= 5 else 2
                })
        
        return sorted(problem_areas, key=lambda x: x["priority"])
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ERROR TRACKING & DIAGNOSIS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def track_error_metric(self, model_name: str, metric_name: str, 
                          value: float, threshold: float):
        """Track model performance metric"""
        status = "good" if value <= threshold else "warning" if value <= threshold * 1.5 else "critical"
        
        metric = ErrorMetric(
            metric_name=metric_name,
            value=value,
            threshold=threshold,
            status=status
        )
        
        self.error_metrics[model_name].append(metric)
        
        if status != "good":
            self.logger.warning(f"âš ï¸  {model_name}.{metric_name}: {value:.3f} (threshold: {threshold:.3f})")
        
        return metric
    
    def diagnose_model_issues(self, model_name: str) -> Dict[str, Any]:
        """Diagnose issues with specific model"""
        if model_name not in self.error_metrics:
            return {"error": f"No metrics for {model_name}"}
        
        metrics = self.error_metrics[model_name]
        
        if not metrics:
            return {"status": "no_issues", "message": "No error metrics tracked"}
        
        # Analyze recent metrics
        recent_metrics = metrics[-20:]  # Last 20 records
        
        critical_metrics = [m for m in recent_metrics if m.status == "critical"]
        warning_metrics = [m for m in recent_metrics if m.status == "warning"]
        
        # Trend analysis
        if len(recent_metrics) >= 2:
            values = [m.value for m in recent_metrics]
            trend = "improving" if values[-1] < np.mean(values[:-1]) else "degrading" if values[-1] > np.mean(values[:-1]) else "stable"
        else:
            trend = "insufficient_data"
        
        diagnosis = {
            "model_name": model_name,
            "critical_metrics": len(critical_metrics),
            "warning_metrics": len(warning_metrics),
            "trend": trend,
            "recent_metrics": [
                {
                    "name": m.metric_name,
                    "value": m.value,
                    "threshold": m.threshold,
                    "status": m.status
                }
                for m in recent_metrics[-5:]
            ],
            "recommended_actions": self._recommend_improvement_actions(
                model_name, critical_metrics, warning_metrics, trend
            )
        }
        
        return diagnosis
    
    def _recommend_improvement_actions(self, model_name: str,
                                      critical_metrics: List[ErrorMetric],
                                      warning_metrics: List[ErrorMetric],
                                      trend: str) -> List[str]:
        """Recommend improvement actions based on diagnosis"""
        recommendations = []
        
        if critical_metrics:
            recommendations.append("URGENT: Retrain model with additional data")
            recommendations.append("URGENT: Review and correct mislabeled training examples")
        
        if warning_metrics:
            recommendations.append("Review feature engineering - may need new features")
            recommendations.append("Consider ensemble methods to improve robustness")
        
        if trend == "degrading":
            recommendations.append("Model performance degrading - collect new training data")
            recommendations.append("Check for data drift in production")
        
        if not recommendations:
            recommendations.append("Model stable - maintain current parameters")
        
        return recommendations
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # IMPROVEMENT PLANNING & EXECUTION
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def plan_improvement(self, model_name: str, 
                        strategy: ImprovementStrategy,
                        description: str,
                        expected_improvement_percent: float = 5.0) -> ImprovementAction:
        """Plan improvement action"""
        action = ImprovementAction(
            strategy=strategy,
            description=description,
            expected_improvement=expected_improvement_percent
        )
        
        self.improvement_actions.append(action)
        self.logger.info(f"ğŸ“‹ Planned improvement: {strategy.value} - {description}")
        
        return action
    
    def execute_feedback_loop_improvement(self, model_name: str,
                                         trainer: Any) -> Dict[str, Any]:
        """Execute feedback-driven improvement"""
        self.logger.info(f"ğŸ”„ Executing feedback loop improvement for {model_name}")
        
        # Analyze recent feedback
        analysis = self.analyze_feedback(lookback_hours=24)
        
        if analysis.get("feedback_count", 0) < 5:
            return {
                "status": "insufficient_feedback",
                "message": "Need more feedback data to drive improvements"
            }
        
        # Extract corrected examples from feedback
        corrected_examples = [
            (f.predicted_value, f.ground_truth)
            for f in self.feedback_log[-100:]
            if f.feedback_type == "incorrect"
        ]
        
        if not corrected_examples:
            return {
                "status": "no_corrections",
                "message": "No incorrect predictions to learn from"
            }
        
        return {
            "status": "executed",
            "action": "feedback_loop",
            "corrections_processed": len(corrected_examples),
            "next_step": "Retrain model with corrected data"
        }
    
    def execute_error_analysis_improvement(self, model_name: str) -> Dict[str, Any]:
        """Execute error analysis improvement"""
        self.logger.info(f"ğŸ” Executing error analysis for {model_name}")
        
        diagnosis = self.diagnose_model_issues(model_name)
        
        return {
            "status": "executed",
            "action": "error_analysis",
            "diagnosis": diagnosis,
            "recommendations": diagnosis.get("recommended_actions", [])
        }
    
    def execute_data_augmentation_improvement(self, model_name: str,
                                            base_samples: List[np.ndarray]) -> Dict[str, Any]:
        """Generate synthetic training data to improve model"""
        self.logger.info(f"ğŸ”§ Executing data augmentation for {model_name}")
        
        augmented_samples = []
        
        # Simple augmentation: add noise to existing samples
        for sample in base_samples[:10]:  # Augment first 10 samples
            # Add small Gaussian noise
            augmented = sample + np.random.normal(0, 0.01, sample.shape)
            augmented_samples.append(augmented)
            
            # Rotation/translation (for spatial data)
            rotated = sample * np.random.uniform(0.95, 1.05)
            augmented_samples.append(rotated)
        
        return {
            "status": "executed",
            "action": "data_augmentation",
            "original_samples": len(base_samples),
            "augmented_samples": len(augmented_samples),
            "total_training_samples": len(base_samples) + len(augmented_samples),
            "recommendation": "Retrain with augmented data"
        }
    
    def set_baseline_performance(self, model_name: str, metrics: Dict[str, float]):
        """Set baseline performance for comparison"""
        self.performance_baseline[model_name] = metrics
        self.logger.info(f"ğŸ“Š Baseline set for {model_name}: {metrics}")
    
    def calculate_improvement(self, model_name: str, current_metrics: Dict[str, float]) -> Dict[str, Any]:
        """Calculate improvement against baseline"""
        if model_name not in self.performance_baseline:
            return {"error": f"No baseline for {model_name}"}
        
        baseline = self.performance_baseline[model_name]
        
        improvements = {}
        for metric_name, current_value in current_metrics.items():
            if metric_name in baseline:
                baseline_value = baseline[metric_name]
                if baseline_value != 0:
                    percent_change = ((current_value - baseline_value) / abs(baseline_value)) * 100
                else:
                    percent_change = 0
                
                improvements[metric_name] = {
                    "baseline": baseline_value,
                    "current": current_value,
                    "improvement_percent": percent_change,
                    "direction": "better" if percent_change > 0 else "worse" if percent_change < 0 else "unchanged"
                }
        
        return {
            "model": model_name,
            "metrics_improvement": improvements,
            "overall_improvement": float(np.mean([m["improvement_percent"] for m in improvements.values()]))
        }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # MONITORING & REPORTING
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def get_improvement_status(self) -> Dict[str, Any]:
        """Get overall improvement status"""
        return {
            "feedback_collected": len(self.feedback_log),
            "improvement_actions_planned": len(self.improvement_actions),
            "active_improvements": len([a for a in self.improvement_actions if a.status == "in_progress"]),
            "completed_improvements": len([a for a in self.improvement_actions if a.status == "completed"]),
            "models_tracked": len(self.error_metrics),
            "critical_issues": sum(
                len([m for m in metrics if m.status == "critical"])
                for metrics in self.error_metrics.values()
            ),
            "recent_improvements": [
                {
                    "strategy": a.strategy.value,
                    "description": a.description,
                    "status": a.status,
                    "expected_improvement": f"{a.expected_improvement:.1f}%",
                    "actual_improvement": f"{a.actual_improvement:.1f}%" if a.actual_improvement else "pending"
                }
                for a in self.improvement_actions[-10:]
            ]
        }
    
    def generate_improvement_report(self) -> Dict[str, Any]:
        """Generate comprehensive improvement report"""
        report = {
            "timestamp": datetime.utcnow().isoformat(),
            "system_status": self.get_improvement_status(),
            "recent_feedback_analysis": self.analyze_feedback(lookback_hours=72),
            "model_diagnostics": {
                model_name: self.diagnose_model_issues(model_name)
                for model_name in self.error_metrics.keys()
            },
            "improvement_priorities": self._rank_improvement_priorities(),
            "recommendations": self._generate_recommendations()
        }
        
        return report
    
    def _rank_improvement_priorities(self) -> List[Dict[str, Any]]:
        """Rank improvements by priority"""
        priorities = []
        
        # Check critical metrics
        for model_name, metrics in self.error_metrics.items():
            critical = [m for m in metrics[-10:] if m.status == "critical"]
            if critical:
                priorities.append({
                    "model": model_name,
                    "priority": "critical",
                    "reason": f"{len(critical)} critical metrics",
                    "score": len(critical) * 100
                })
        
        # Check feedback
        recent_fb = [f for f in self.feedback_log[-50:] if f.feedback_type == "incorrect"]
        if len(recent_fb) > 10:
            priorities.append({
                "model": "all",
                "priority": "high",
                "reason": f"{len(recent_fb)} incorrect predictions in last 50 feedback",
                "score": len(recent_fb) * 10
            })
        
        return sorted(priorities, key=lambda x: x["score"], reverse=True)
    
    def _generate_recommendations(self) -> List[str]:
        """Generate actionable recommendations"""
        recommendations = []
        
        # Feedback-based
        if len(self.feedback_log) > 100:
            recommendations.append("Retrain models using accumulated feedback data")
        
        # Error-based
        total_critical = sum(
            len([m for m in metrics if m.status == "critical"])
            for metrics in self.error_metrics.values()
        )
        if total_critical > 0:
            recommendations.append(f"Address {total_critical} critical metrics immediately")
        
        # Performance-based
        if self.improvement_actions:
            completed = len([a for a in self.improvement_actions if a.status == "completed"])
            if completed > 0:
                avg_improvement = float(np.mean([
                    a.actual_improvement for a in self.improvement_actions
                    if a.status == "completed" and a.actual_improvement
                ]))
                recommendations.append(f"Improvements are working - average gain: {avg_improvement:.1f}%")
        
        return recommendations if recommendations else ["System performing well - continue monitoring"]
