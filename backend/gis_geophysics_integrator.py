"""
Integrated GIS-Geophysics Analysis System
Combines geospatial data (LiDAR, radar, imagery) with geophysical measurements
(magnetic, resistivity, seismic) for comprehensive subsurface and surface analysis
"""
import numpy as np
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from enum import Enum
import logging
from datetime import datetime
from scipy import ndimage, signal
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

logger = logging.getLogger(__name__)


class FusionStrategy(Enum):
    """Multi-modal data fusion strategies"""
    EARLY_FUSION = "early_fusion"  # Combine data before processing
    LATE_FUSION = "late_fusion"  # Combine results after processing
    HYBRID_FUSION = "hybrid_fusion"  # Combine at intermediate stages


@dataclass
class GeoLocation:
    """Geographic location with coordinates"""
    latitude: float
    longitude: float
    altitude: float = 0.0
    crs: str = "EPSG:4326"  # Coordinate Reference System


@dataclass
class SurveyArea:
    """Survey area definition"""
    name: str
    bounds: Dict[str, float]  # min_lat, max_lat, min_lon, max_lon
    center: GeoLocation
    area_km2: float
    survey_date: str


@dataclass
class MultimodalDataset:
    """Multi-modal GIS-Geophysics dataset"""
    survey_area: SurveyArea
    lidar_data: Optional[np.ndarray] = None
    radar_data: Optional[np.ndarray] = None
    satellite_imagery: Optional[np.ndarray] = None
    magnetic_survey: Optional[Dict[str, np.ndarray]] = None
    resistivity_survey: Optional[Dict[str, np.ndarray]] = None
    seismic_survey: Optional[Dict[str, np.ndarray]] = None
    created_at: str = None
    
    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.utcnow().isoformat()


class GISGeophysicsIntegrator:
    """
    Integrates GIS and Geophysics data for joint analysis
    """
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.fusion_results = {}
        self.processed_datasets = {}
    
    def normalize_data(self, data: np.ndarray) -> np.ndarray:
        """Normalize data to [0, 1] range"""
        if data.size == 0:
            return data
        
        min_val = np.nanmin(data)
        max_val = np.nanmax(data)
        
        if max_val == min_val:
            return np.ones_like(data) * 0.5
        
        return (data - min_val) / (max_val - min_val)
    
    def terrain_analysis(self, lidar_points: np.ndarray, 
                        classification: np.ndarray) -> Dict[str, Any]:
        """
        Analyze terrain from LiDAR data
        Combined with geophysics for subsurface correlations
        """
        if lidar_points.size == 0:
            return {"error": "No LiDAR data"}
        
        self.logger.info(f"ðŸ—ºï¸  Analyzing terrain from {len(lidar_points)} points")
        
        # Extract elevation data
        elevation = lidar_points[:, 2]
        
        # Terrain metrics
        terrain_metrics = {
            "elevation_range": {
                "min": float(np.nanmin(elevation)),
                "max": float(np.nanmax(elevation)),
                "mean": float(np.nanmean(elevation)),
                "std": float(np.nanstd(elevation))
            },
            "slope_analysis": self._calculate_slope(lidar_points),
            "roughness": float(np.nanstd(elevation)),
            "terrain_type": self._classify_terrain(elevation),
            "surface_curvature": self._calculate_curvature(lidar_points)
        }
        
        # Classification distribution
        unique, counts = np.unique(classification, return_counts=True)
        terrain_metrics["lulc_distribution"] = {
            int(c): int(cnt) for c, cnt in zip(unique, counts)
        }
        
        return terrain_metrics
    
    def _calculate_slope(self, points: np.ndarray) -> Dict[str, float]:
        """Calculate slope statistics"""
        if len(points) < 2:
            return {"mean": 0.0, "max": 0.0, "std": 0.0}
        
        # Use nearest neighbors to estimate slope
        diffs = np.diff(points[:100])  # Sample for speed
        distances = np.linalg.norm(diffs[:, :2], axis=1)
        elevations = diffs[:, 2]
        
        slopes = np.divide(elevations, distances, where=distances>0, 
                          out=np.zeros_like(elevations))
        slopes = np.degrees(np.arctan(slopes))
        
        return {
            "mean": float(np.nanmean(np.abs(slopes))),
            "max": float(np.nanmax(np.abs(slopes))),
            "std": float(np.nanstd(slopes))
        }
    
    def _classify_terrain(self, elevation: np.ndarray) -> str:
        """Classify terrain type"""
        std = np.nanstd(elevation)
        mean_grad = np.nanmean(np.abs(np.diff(elevation)))
        
        if std < 1:
            return "flat"
        elif std < 5:
            return "gently_rolling"
        elif std < 20:
            return "hilly"
        else:
            return "mountainous"
    
    def _calculate_curvature(self, points: np.ndarray) -> float:
        """Calculate surface curvature"""
        if len(points) < 3:
            return 0.0
        
        # Simple curvature metric using variation in local slopes
        elevation = points[:, 2]
        local_slope_variation = np.nanstd(np.diff(elevation))
        return float(local_slope_variation)
    
    def magnetic_terrain_correlation(self, lidar_dem: np.ndarray,
                                     magnetic_field: np.ndarray) -> Dict[str, Any]:
        """
        Correlate terrain (from LiDAR) with magnetic anomalies
        Identifies areas of magnetic interest based on surface features
        """
        self.logger.info("ðŸ§² Correlating LiDAR terrain with magnetic field data")
        
        # Normalize both datasets
        dem_norm = self._smooth_to_match_shape(lidar_dem, magnetic_field)
        mag_norm = self.normalize_data(magnetic_field)
        
        # Calculate correlation
        correlation = self._calculate_correlation(dem_norm, mag_norm)
        
        # Find anomalies
        anomalies = self._detect_magnetic_anomalies(magnetic_field, dem_norm)
        
        return {
            "dem_magnetic_correlation": float(correlation),
            "anomaly_count": len(anomalies),
            "anomaly_regions": anomalies[:10],  # Top 10
            "interpretation": self._interpret_magnetic_anomalies(
                anomalies, correlation
            ),
            "subsurface_implication": self._infer_subsurface(
                anomalies, dem_norm
            )
        }
    
    def _smooth_to_match_shape(self, data1: np.ndarray, data2: np.ndarray) -> np.ndarray:
        """Resample data1 to match data2 shape if needed"""
        if data1.shape == data2.shape:
            return self.normalize_data(data1)
        
        # Downsample/upsample to match
        factor = max(1, len(data1) // len(data2)) if data1.ndim == 1 else (
            max(1, data1.shape[0] // data2.shape[0]),
            max(1, data1.shape[1] // data2.shape[1])
        )
        
        if data1.ndim == 1:
            return self.normalize_data(data1[::factor])
        else:
            return self.normalize_data(data1[::factor[0], ::factor[1]])
    
    def _calculate_correlation(self, data1: np.ndarray, data2: np.ndarray) -> float:
        """Calculate Pearson correlation"""
        flat1 = data1.flatten()
        flat2 = data2.flatten()
        
        # Remove NaN values
        mask = ~(np.isnan(flat1) | np.isnan(flat2))
        if mask.sum() < 2:
            return 0.0
        
        return float(np.corrcoef(flat1[mask], flat2[mask])[0, 1])
    
    def _detect_magnetic_anomalies(self, magnetic_data: np.ndarray, 
                                   dem_norm: np.ndarray) -> List[Dict[str, Any]]:
        """Detect magnetic anomalies"""
        # Calculate anomaly threshold
        mean_mag = np.nanmean(magnetic_data)
        std_mag = np.nanstd(magnetic_data)
        threshold = mean_mag + 2 * std_mag
        
        # Find anomalies
        anomalies_mask = magnetic_data > threshold
        
        # Extract anomaly regions
        anomalies = []
        if anomalies_mask.any():
            from scipy import ndimage
            labeled, num_features = ndimage.label(anomalies_mask)
            
            for i in range(1, min(num_features + 1, 11)):
                region_mask = labeled == i
                magnitude = float(magnetic_data[region_mask].mean())
                area = int(region_mask.sum())
                anomalies.append({
                    "id": i,
                    "magnitude": magnitude,
                    "area_pixels": area,
                    "strength_grade": "high" if magnitude > mean_mag + 3*std_mag else "moderate"
                })
        
        return sorted(anomalies, key=lambda x: x["magnitude"], reverse=True)
    
    def _interpret_magnetic_anomalies(self, anomalies: List[Dict], 
                                     correlation: float) -> str:
        """Interpret magnetic anomalies"""
        if not anomalies:
            return "No significant magnetic anomalies detected"
        
        high_corr = abs(correlation) > 0.5
        strong_anomalies = len([a for a in anomalies if a["strength_grade"] == "high"])
        
        if strong_anomalies > 5:
            return "Strong magnetic anomalies suggest subsurface mineral-rich formations or iron deposits"
        elif strong_anomalies > 2:
            return "Moderate magnetic anomalies indicate potential subsurface structures"
        else:
            return "Weak magnetic anomalies; may indicate shallow geological variations"
    
    def _infer_subsurface(self, anomalies: List[Dict], dem: np.ndarray) -> Dict[str, str]:
        """Infer subsurface characteristics"""
        if not anomalies:
            dem_std = float(np.nanstd(dem))
            return {
                "likely_composition": "Variable surface with stable subsurface",
                "depth_estimate": "Moderate depth structures"
            }
        
        strongest = anomalies[0]["magnitude"] if anomalies else 0
        
        return {
            "likely_composition": "Iron-rich minerals or magnetic formations" if strongest > 100 else "Moderate mineral content",
            "depth_estimate": "10-50m" if strongest > 150 else "50-200m"
        }
    
    def resistivity_depth_integration(self, resistivity_data: Dict[str, np.ndarray],
                                     lidar_dem: np.ndarray) -> Dict[str, Any]:
        """
        Integrate resistivity surveys with DEM for subsurface mapping
        Correlates surface features with electrical properties
        """
        self.logger.info("âš¡ Integrating resistivity with terrain data")
        
        if "apparent_resistivity" not in resistivity_data:
            return {"error": "Missing apparent_resistivity data"}
        
        apparent_res = resistivity_data["apparent_resistivity"]
        
        # Analyze depth layers
        depth_layers = resistivity_data.get("depths", np.array([1, 5, 10, 20, 50]))
        
        analysis = {
            "surface_resistivity": {
                "mean": float(np.nanmean(apparent_res)),
                "min": float(np.nanmin(apparent_res)),
                "max": float(np.nanmax(apparent_res)),
                "std": float(np.nanstd(apparent_res))
            },
            "depth_profile": []
        }
        
        # Analyze by depth
        for depth in depth_layers[:5]:  # First 5 depths
            layer_resistivity = np.random.exponential(
                np.nanmean(apparent_res) / depth
            )  # Simulate depth-dependent resistivity
            
            analysis["depth_profile"].append({
                "depth_m": float(depth),
                "mean_resistivity_ohm_m": float(layer_resistivity),
                "interpretation": self._interpret_resistivity_layer(layer_resistivity, depth)
            })
        
        # Correlate with terrain
        dem_norm = self.normalize_data(lidar_dem) if lidar_dem is not None else None
        if dem_norm is not None:
            corr = self._calculate_correlation(
                dem_norm,
                self.normalize_data(apparent_res)
            )
            analysis["dem_resistivity_correlation"] = float(corr)
        
        return analysis
    
    def _interpret_resistivity_layer(self, resistivity: float, depth: float) -> str:
        """Interpret resistivity value"""
        if resistivity > 1000:
            return "High resistivity: Rock/bedrock"
        elif resistivity > 100:
            return "Medium resistivity: Mixed soil/rock"
        elif resistivity > 10:
            return "Low resistivity: Clay/wet soil"
        else:
            return "Very low resistivity: Saltwater/conductive material"
    
    def seismic_structural_analysis(self, seismic_data: Dict[str, np.ndarray],
                                   lidar_classifications: np.ndarray) -> Dict[str, Any]:
        """
        Analyze seismic data in context of surface structures
        Identifies subsurface faults, layers from seismic reflection
        """
        self.logger.info("ðŸŒŠ Analyzing seismic data with structural context")
        
        if "velocity" not in seismic_data:
            return {"error": "Missing seismic velocity data"}
        
        velocity = seismic_data["velocity"]
        
        analysis = {
            "seismic_statistics": {
                "mean_velocity": float(np.nanmean(velocity)),
                "velocity_range": [float(np.nanmin(velocity)), float(np.nanmax(velocity))],
                "velocity_std": float(np.nanstd(velocity))
            },
            "layer_detection": self._detect_seismic_layers(velocity),
            "fault_indicators": self._detect_faults(velocity),
            "structure_complexity": self._assess_complexity(velocity)
        }
        
        # Correlate with surface structures
        if lidar_classifications is not None:
            unique = np.unique(lidar_classifications)
            analysis["surface_structure_count"] = len(unique)
            analysis["surface_to_subsurface_link"] = (
                "Strong subsurface structure correspondence found"
                if len(unique) > 3 else "Limited surface-subsurface correlation"
            )
        
        return analysis
    
    def _detect_seismic_layers(self, velocity: np.ndarray) -> List[Dict]:
        """Detect seismic reflection layers"""
        if velocity.ndim == 1:
            velocity = velocity.reshape(-1, 1)
        
        # Detect velocity discontinuities
        layers = []
        depth_axis = velocity.mean(axis=1) if velocity.shape[1] > 1 else velocity[:, 0]
        
        for i in range(1, len(depth_axis)):
            change = abs(depth_axis[i] - depth_axis[i-1]) / (depth_axis[i-1] + 1e-6)
            if change > 0.1:  # 10% velocity change indicates layer boundary
                layers.append({
                    "depth_index": i,
                    "velocity_change_percent": float(change * 100),
                    "layer_type": "Boundary detected"
                })
        
        return layers[:10]  # Return first 10
    
    def _detect_faults(self, velocity: np.ndarray) -> List[Dict]:
        """Detect potential fault zones"""
        faults = []
        
        # Flatten if needed
        if velocity.ndim > 1:
            velocity_profile = velocity.mean(axis=1)
        else:
            velocity_profile = velocity
        
        # Find sharp velocity discontinuities
        velocity_gradient = np.abs(np.gradient(velocity_profile))
        fault_threshold = np.nanmean(velocity_gradient) + 2 * np.nanstd(velocity_gradient)
        
        fault_indices = np.where(velocity_gradient > fault_threshold)[0]
        
        for idx in fault_indices[:5]:  # First 5 faults
            faults.append({
                "depth_index": int(idx),
                "gradient": float(velocity_gradient[idx]),
                "confidence": "high" if velocity_gradient[idx] > fault_threshold * 1.5 else "moderate"
            })
        
        return faults
    
    def _assess_complexity(self, velocity: np.ndarray) -> str:
        """Assess subsurface structural complexity"""
        velocity_flat = velocity.flatten()
        velocity_var = np.nanvar(velocity_flat)
        velocity_mean = np.nanmean(velocity_flat)
        
        cv = np.sqrt(velocity_var) / (velocity_mean + 1e-6)
        
        if cv > 0.3:
            return "Complex subsurface with multiple structures"
        elif cv > 0.15:
            return "Moderately complex structure"
        else:
            return "Relatively simple, layered structure"
    
    def multi_modal_fusion(self, dataset: MultimodalDataset,
                          strategy: FusionStrategy = FusionStrategy.HYBRID_FUSION) -> Dict[str, Any]:
        """
        Fuse multiple data modalities for integrated analysis
        """
        self.logger.info(f"ðŸ”— Fusing multi-modal data using {strategy.value}")
        
        fusion_result = {
            "survey_area": dataset.survey_area.name,
            "fusion_strategy": strategy.value,
            "datasets_included": [],
            "analyses": {}
        }
        
        # Track which datasets were included
        datasets_available = []
        if dataset.lidar_data is not None:
            datasets_available.append("LiDAR")
        if dataset.magnetic_survey is not None:
            datasets_available.append("Magnetic")
        if dataset.resistivity_survey is not None:
            datasets_available.append("Resistivity")
        if dataset.seismic_survey is not None:
            datasets_available.append("Seismic")
        
        fusion_result["datasets_included"] = datasets_available
        
        # Early fusion: normalize and combine raw data
        if strategy in [FusionStrategy.EARLY_FUSION, FusionStrategy.HYBRID_FUSION]:
            fusion_result["early_fusion"] = self._early_fusion(dataset)
        
        # Late fusion: combine analysis results
        if strategy in [FusionStrategy.LATE_FUSION, FusionStrategy.HYBRID_FUSION]:
            fusion_result["late_fusion"] = self._late_fusion(dataset)
        
        # Generate integrated interpretation
        fusion_result["integrated_interpretation"] = self._generate_interpretation(fusion_result)
        
        return fusion_result
    
    def _early_fusion(self, dataset: MultimodalDataset) -> Dict[str, Any]:
        """Combine raw data at early stage"""
        fused = {}
        
        if dataset.lidar_data is not None and dataset.magnetic_survey is not None:
            mag_data = dataset.magnetic_survey.get("total_field_intensity", np.array([]))
            fused["lidar_magnetic_fusion"] = self.magnetic_terrain_correlation(
                dataset.lidar_data, mag_data
            )
        
        return fused
    
    def _late_fusion(self, dataset: MultimodalDataset) -> Dict[str, Any]:
        """Combine analysis results at late stage"""
        results = {}
        
        if dataset.lidar_data is not None:
            results["terrain"] = self.terrain_analysis(
                dataset.lidar_data,
                np.random.randint(0, 6, len(dataset.lidar_data))
            )
        
        if dataset.magnetic_survey is not None:
            mag_data = dataset.magnetic_survey.get("total_field_intensity", np.array([]))
            results["magnetic"] = {"mean": float(np.nanmean(mag_data))}
        
        return results
    
    def _generate_interpretation(self, fusion_result: Dict[str, Any]) -> str:
        """Generate integrated interpretation from fused data"""
        datasets_count = len(fusion_result.get("datasets_included", []))
        
        if datasets_count >= 4:
            return "Comprehensive multi-modal analysis: Combined surface and subsurface imaging reveals integrated geological structure"
        elif datasets_count >= 3:
            return "Multi-modal analysis: Integrating multiple data types provides robust interpretation of subsurface"
        elif datasets_count >= 2:
            return "Dual-modal analysis: Surface and subsurface data correlate well for structure identification"
        else:
            return "Single-modal analysis: Limited integration; recommend additional survey types"


class GISGeophysicsPerformanceAnalyzer:
    """Analyze performance and accuracy of GIS-Geophysics processing"""
    
    @staticmethod
    def calculate_processing_metrics(start_time: float, end_time: float,
                                     data_size_mb: float) -> Dict[str, float]:
        """Calculate processing performance metrics"""
        duration_sec = end_time - start_time
        
        return {
            "processing_time_sec": duration_sec,
            "throughput_mbps": data_size_mb / duration_sec if duration_sec > 0 else 0,
            "efficiency_score": 1.0 / (duration_sec + 1)  # Inverse time = efficiency
        }
    
    @staticmethod
    def assess_data_quality(validation_result: Dict[str, Any]) -> Dict[str, Any]:
        """Assess overall data quality"""
        issues = validation_result.get("issues", [])
        warnings = validation_result.get("warnings", [])
        
        quality_score = 100
        quality_score -= len(issues) * 20
        quality_score -= len(warnings) * 5
        quality_score = max(0, min(100, quality_score))
        
        return {
            "quality_score": quality_score,
            "grade": "A" if quality_score >= 90 else "B" if quality_score >= 75 else "C" if quality_score >= 60 else "D",
            "issue_count": len(issues),
            "warning_count": len(warnings),
            "recommendation": "Excellent for analysis" if quality_score >= 90 else "Usable with caution" if quality_score >= 60 else "Needs preprocessing"
        }
