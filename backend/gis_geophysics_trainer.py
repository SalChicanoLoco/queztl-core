"""
GIS-Geophysics Training System
ML-based training for improved geospatial analysis and subsurface characterization
"""
import numpy as np
from typing import Dict, List, Tuple, Optional, Any, Callable
from dataclasses import dataclass
from enum import Enum
import logging
from datetime import datetime
import json
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, r2_score

logger = logging.getLogger(__name__)


class TaskType(Enum):
    """GIS-Geophysics machine learning task types"""
    CLASSIFICATION = "classification"  # Classify terrain, rock types, etc
    REGRESSION = "regression"  # Predict depth, resistivity, velocity
    ANOMALY_DETECTION = "anomaly_detection"  # Find anomalies
    CLUSTERING = "clustering"  # Cluster similar features


@dataclass
class TrainingExample:
    """Single training example"""
    features: np.ndarray  # Input features
    label: Any  # Target label/value
    metadata: Dict[str, Any] = None
    weight: float = 1.0  # Importance weight


@dataclass
class TrainingDataset:
    """Collection of training examples"""
    task_type: TaskType
    examples: List[TrainingExample]
    feature_names: List[str]
    label_name: str
    created_at: str = None
    
    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.utcnow().isoformat()
    
    def to_arrays(self) -> Tuple[np.ndarray, np.ndarray]:
        """Convert to feature and label arrays"""
        X = np.array([ex.features for ex in self.examples])
        y = np.array([ex.label for ex in self.examples])
        return X, y


@dataclass
class ModelPerformance:
    """Model performance metrics"""
    task_type: TaskType
    train_score: float
    test_score: float
    validation_score: Optional[float] = None
    metrics: Dict[str, float] = None
    confusion_matrix: Optional[np.ndarray] = None
    feature_importance: Optional[Dict[str, float]] = None
    training_time_sec: float = 0.0


class GISGeophysicsModel:
    """Base model for GIS-Geophysics ML tasks"""
    
    def __init__(self, task_type: TaskType):
        self.task_type = task_type
        self.model = None
        self.scaler = StandardScaler()
        self.performance = None
        self.trained = False
        self.feature_names = []
    
    def train(self, dataset: TrainingDataset, test_size: float = 0.2) -> ModelPerformance:
        """Train model on dataset"""
        logger.info(f"ðŸŽ“ Training {self.task_type.value} model with {len(dataset.examples)} examples")
        
        import time
        start_time = time.time()
        
        X, y = dataset.to_arrays()
        self.feature_names = dataset.feature_names
        
        # Normalize features
        X_scaled = self.scaler.fit_transform(X)
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=test_size, random_state=42
        )
        
        # Train model
        if self.task_type == TaskType.CLASSIFICATION:
            self.model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
        elif self.task_type == TaskType.REGRESSION:
            self.model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
        else:
            logger.warning(f"Task type {self.task_type} not fully supported yet")
            return None
        
        self.model.fit(X_train, y_train)
        
        # Evaluate
        train_score = self.model.score(X_train, y_train)
        test_score = self.model.score(X_test, y_test)
        
        # Detailed metrics
        y_pred = self.model.predict(X_test)
        metrics = self._calculate_detailed_metrics(y_test, y_pred)
        
        training_time = time.time() - start_time
        
        self.performance = ModelPerformance(
            task_type=self.task_type,
            train_score=train_score,
            test_score=test_score,
            metrics=metrics,
            feature_importance=dict(zip(
                dataset.feature_names,
                self.model.feature_importances_
            )),
            training_time_sec=training_time
        )
        
        self.trained = True
        logger.info(f"âœ… Model trained: Train={train_score:.3f}, Test={test_score:.3f}")
        
        return self.performance
    
    def _calculate_detailed_metrics(self, y_true: np.ndarray, 
                                   y_pred: np.ndarray) -> Dict[str, float]:
        """Calculate detailed performance metrics"""
        if self.task_type == TaskType.CLASSIFICATION:
            return {
                "accuracy": float(accuracy_score(y_true, y_pred)),
                "precision": float(precision_score(y_true, y_pred, average='weighted', zero_division=0)),
                "recall": float(recall_score(y_true, y_pred, average='weighted', zero_division=0)),
                "f1": float(f1_score(y_true, y_pred, average='weighted', zero_division=0))
            }
        elif self.task_type == TaskType.REGRESSION:
            mse = mean_squared_error(y_true, y_pred)
            return {
                "mse": float(mse),
                "rmse": float(np.sqrt(mse)),
                "mae": float(np.mean(np.abs(y_true - y_pred))),
                "r2": float(r2_score(y_true, y_pred))
            }
        return {}
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """Make predictions"""
        if not self.trained or self.model is None:
            raise ValueError("Model not trained yet")
        
        X_scaled = self.scaler.transform(X)
        return self.model.predict(X_scaled)
    
    def predict_proba(self, X: np.ndarray) -> Optional[np.ndarray]:
        """Get prediction probabilities (for classification)"""
        if not hasattr(self.model, 'predict_proba'):
            return None
        
        X_scaled = self.scaler.transform(X)
        return self.model.predict_proba(X_scaled)


class GISGeophysicsTrainer:
    """High-level training orchestrator for GIS-Geophysics tasks"""
    
    def __init__(self):
        self.models: Dict[str, GISGeophysicsModel] = {}
        self.datasets: Dict[str, TrainingDataset] = {}
        self.training_history: List[Dict[str, Any]] = []
    
    def create_terrain_classifier(self) -> GISGeophysicsModel:
        """Create model to classify terrain types"""
        logger.info("ðŸ“ Creating terrain classification model")
        return GISGeophysicsModel(TaskType.CLASSIFICATION)
    
    def create_depth_predictor(self) -> GISGeophysicsModel:
        """Create model to predict subsurface depth to features"""
        logger.info("ðŸ“ Creating depth prediction model")
        return GISGeophysicsModel(TaskType.REGRESSION)
    
    def create_anomaly_detector(self) -> GISGeophysicsModel:
        """Create model to detect geophysical anomalies"""
        logger.info("ðŸ” Creating anomaly detection model")
        # Would use clustering or isolation forest
        return GISGeophysicsModel(TaskType.ANOMALY_DETECTION)
    
    def train_terrain_classifier(self, lidar_samples: List[np.ndarray],
                                 terrain_labels: List[int]) -> ModelPerformance:
        """Train model to classify terrain from LiDAR data"""
        
        # Create training dataset
        examples = [
            TrainingExample(
                features=sample,
                label=label
            )
            for sample, label in zip(lidar_samples, terrain_labels)
        ]
        
        dataset = TrainingDataset(
            task_type=TaskType.CLASSIFICATION,
            examples=examples,
            feature_names=[f"lidar_feature_{i}" for i in range(lidar_samples[0].shape[0])],
            label_name="terrain_type"
        )
        
        self.datasets["terrain_classifier"] = dataset
        
        # Train model
        model = self.create_terrain_classifier()
        performance = model.train(dataset)
        
        self.models["terrain_classifier"] = model
        
        # Log history
        self.training_history.append({
            "timestamp": datetime.utcnow().isoformat(),
            "task": "terrain_classification",
            "performance": {
                "train_score": performance.train_score,
                "test_score": performance.test_score,
                "metrics": performance.metrics
            }
        })
        
        return performance
    
    def train_depth_predictor(self, geophysics_features: List[np.ndarray],
                             measured_depths: List[float]) -> ModelPerformance:
        """Train model to predict depth from geophysical measurements"""
        
        # Create training dataset
        examples = [
            TrainingExample(
                features=features,
                label=depth
            )
            for features, depth in zip(geophysics_features, measured_depths)
        ]
        
        dataset = TrainingDataset(
            task_type=TaskType.REGRESSION,
            examples=examples,
            feature_names=[f"geophys_feature_{i}" for i in range(geophysics_features[0].shape[0])],
            label_name="depth_m"
        )
        
        self.datasets["depth_predictor"] = dataset
        
        # Train model
        model = self.create_depth_predictor()
        performance = model.train(dataset)
        
        self.models["depth_predictor"] = model
        
        # Log history
        self.training_history.append({
            "timestamp": datetime.utcnow().isoformat(),
            "task": "depth_prediction",
            "performance": {
                "train_score": performance.train_score,
                "test_score": performance.test_score,
                "metrics": performance.metrics
            }
        })
        
        return performance
    
    def train_lithology_classifier(self, resistivity_profiles: List[np.ndarray],
                                   lithology_labels: List[str]) -> ModelPerformance:
        """Train model to classify rock types from resistivity data"""
        
        # Encode labels
        label_encoder = {label: i for i, label in enumerate(set(lithology_labels))}
        encoded_labels = [label_encoder[label] for label in lithology_labels]
        
        examples = [
            TrainingExample(
                features=profile,
                label=label,
                metadata={"lithology": lithology_labels[i]}
            )
            for i, (profile, label) in enumerate(zip(resistivity_profiles, encoded_labels))
        ]
        
        dataset = TrainingDataset(
            task_type=TaskType.CLASSIFICATION,
            examples=examples,
            feature_names=[f"resistivity_level_{i}" for i in range(resistivity_profiles[0].shape[0])],
            label_name="lithology_class"
        )
        
        self.datasets["lithology_classifier"] = dataset
        
        # Train model
        model = GISGeophysicsModel(TaskType.CLASSIFICATION)
        performance = model.train(dataset)
        
        self.models["lithology_classifier"] = model
        
        # Log history
        self.training_history.append({
            "timestamp": datetime.utcnow().isoformat(),
            "task": "lithology_classification",
            "performance": {
                "train_score": performance.train_score,
                "test_score": performance.test_score,
                "metrics": performance.metrics
            },
            "label_mapping": label_encoder
        })
        
        return performance
    
    def get_model(self, model_name: str) -> Optional[GISGeophysicsModel]:
        """Retrieve trained model by name"""
        return self.models.get(model_name)
    
    def get_training_history(self, task: Optional[str] = None) -> List[Dict[str, Any]]:
        """Get training history, optionally filtered by task"""
        if task:
            return [entry for entry in self.training_history if entry.get("task") == task]
        return self.training_history
    
    def export_model_summary(self) -> Dict[str, Any]:
        """Export summary of all trained models"""
        return {
            "trained_models": len(self.models),
            "models": {
                name: {
                    "task_type": model.task_type.value,
                    "trained": model.trained,
                    "performance": {
                        "train_score": model.performance.train_score,
                        "test_score": model.performance.test_score,
                        "metrics": model.performance.metrics
                    } if model.performance else None,
                    "feature_importance": list(
                        sorted(
                            model.performance.feature_importance.items(),
                            key=lambda x: x[1],
                            reverse=True
                        )[:5]
                    ) if model.performance and model.performance.feature_importance else None
                }
                for name, model in self.models.items()
            },
            "total_training_time_sec": sum(
                entry.get("performance", {}).get("training_time_sec", 0)
                for entry in self.training_history
            ),
            "training_history_entries": len(self.training_history)
        }


class ActiveLearningEngine:
    """Active learning for continuous model improvement"""
    
    def __init__(self, trainer: GISGeophysicsTrainer):
        self.trainer = trainer
        self.unlabeled_pool: List[np.ndarray] = []
        self.selected_samples: List[Tuple[np.ndarray, Any]] = []
    
    def add_unlabeled_samples(self, samples: List[np.ndarray]):
        """Add unlabeled samples for active learning"""
        self.unlabeled_pool.extend(samples)
        logger.info(f"Added {len(samples)} unlabeled samples. Pool size: {len(self.unlabeled_pool)}")
    
    def select_most_informative(self, model: GISGeophysicsModel, 
                               num_samples: int = 10) -> List[np.ndarray]:
        """Select most informative samples for labeling"""
        if not self.unlabeled_pool:
            return []
        
        if model.task_type == TaskType.CLASSIFICATION:
            # Use uncertainty sampling
            probas = model.predict_proba(np.array(self.unlabeled_pool))
            
            # Entropy of predictions
            entropies = -np.sum(probas * np.log(probas + 1e-10), axis=1)
            
            # Select samples with highest entropy
            most_uncertain = np.argsort(entropies)[-num_samples:]
            selected = [self.unlabeled_pool[i] for i in most_uncertain]
            
            # Remove selected from pool
            for i in sorted(most_uncertain, reverse=True):
                self.unlabeled_pool.pop(i)
            
            return selected
        else:
            # For regression, use prediction variance
            # Simplified: just return random samples
            import random
            selected = random.sample(self.unlabeled_pool, min(num_samples, len(self.unlabeled_pool)))
            for sample in selected:
                self.unlabeled_pool.remove(sample)
            return selected
    
    def incorporate_labeled_data(self, model_name: str, samples: List[np.ndarray],
                                labels: List[Any]) -> ModelPerformance:
        """Incorporate newly labeled data and retrain model"""
        model = self.trainer.get_model(model_name)
        
        if model is None:
            logger.error(f"Model {model_name} not found")
            return None
        
        # Get existing dataset
        dataset = self.trainer.datasets.get(model_name)
        if dataset is None:
            logger.error(f"Dataset for {model_name} not found")
            return None
        
        # Add new labeled examples
        for sample, label in zip(samples, labels):
            dataset.examples.append(TrainingExample(features=sample, label=label))
        
        # Retrain
        logger.info(f"Retraining {model_name} with {len(samples)} new samples")
        performance = model.train(dataset)
        
        return performance
