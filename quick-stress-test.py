#!/usr/bin/env python3
"""
Quick Stress Test & PDF Report Generator
Tests Queztl Protocol performance and generates visual report
"""

import asyncio
import websockets
import time
import struct
import json
from statistics import mean
from datetime import datetime

# Config
QUEZTL_URL = "ws://localhost:9999"
TEST_REQUESTS = [10, 50, 100, 500]

class QuickTest:
    MAGIC = b'QP'
    MSG_COMMAND = 0x01
    
    def __init__(self):
        self.results = []
    
    def pack(self, msg_type: int, payload: bytes) -> bytes:
        header = struct.pack('!2sBL', self.MAGIC, msg_type, len(payload))
        return header + payload
    
    async def test_batch(self, num_requests):
        """Run a batch of requests"""
        print(f"  Testing {num_requests} requests...")
        
        ws = await websockets.connect(QUEZTL_URL)
        latencies = []
        bytes_sent = 0
        bytes_received = 0
        
        start_time = time.time()
        
        for i in range(num_requests):
            payload = json.dumps({
                "capability": "test",
                "params": {"request_id": i}
            }).encode()
            
            message = self.pack(self.MSG_COMMAND, payload)
            bytes_sent += len(message)
            
            req_start = time.perf_counter()
            await ws.send(message)
            response = await ws.recv()
            latency = (time.perf_counter() - req_start) * 1000
            
            bytes_received += len(response)
            latencies.append(latency)
        
        duration = time.time() - start_time
        await ws.close()
        
        result = {
            "requests": num_requests,
            "duration": duration,
            "avg_latency": mean(latencies),
            "min_latency": min(latencies),
            "max_latency": max(latencies),
            "throughput": num_requests / duration,
            "bytes_sent": bytes_sent,
            "bytes_received": bytes_received,
            "overhead_per_msg": bytes_sent / num_requests
        }
        
        print(f"    âœ“ Avg latency: {result['avg_latency']:.2f}ms")
        print(f"    âœ“ Throughput: {result['throughput']:.0f} req/s")
        
        return result
    
    async def run_all_tests(self):
        """Run all test batches"""
        print("\n" + "="*60)
        print(" âš¡ QUEZTL PROTOCOL STRESS TEST")
        print("="*60)
        
        for num in TEST_REQUESTS:
            result = await self.test_batch(num)
            self.results.append(result)
        
        return self.results

def generate_text_report(results):
    """Generate detailed text report"""
    
    report = []
    report.append("\n" + "="*70)
    report.append(" ğŸ“Š QUEZTL PROTOCOL PERFORMANCE REPORT")
    report.append("="*70)
    report.append(f"\nGenerated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # Summary table
    report.append("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    report.append("â”‚                     PERFORMANCE SUMMARY                         â”‚")
    report.append("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    report.append("â”‚ Requests â”‚ Avg Latency  â”‚ Throughput   â”‚ Overhead    â”‚ Result  â”‚")
    report.append("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    
    for r in results:
        report.append(
            f"â”‚ {r['requests']:8d} â”‚ {r['avg_latency']:10.2f}ms â”‚ "
            f"{r['throughput']:10.0f}/s â”‚ {r['overhead_per_msg']:9.0f}B â”‚ "
            f"{'âœ… PASS' if r['avg_latency'] < 50 else 'âš ï¸ SLOW':7s} â”‚"
        )
    
    report.append("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    
    # Detailed metrics
    report.append("\n" + "â”€"*70)
    report.append(" DETAILED METRICS")
    report.append("â”€"*70 + "\n")
    
    for i, r in enumerate(results, 1):
        report.append(f"Test {i}: {r['requests']} Requests")
        report.append(f"  â€¢ Duration:        {r['duration']:.2f}s")
        report.append(f"  â€¢ Avg Latency:     {r['avg_latency']:.2f}ms")
        report.append(f"  â€¢ Min Latency:     {r['min_latency']:.2f}ms")
        report.append(f"  â€¢ Max Latency:     {r['max_latency']:.2f}ms")
        report.append(f"  â€¢ Throughput:      {r['throughput']:.0f} req/s")
        report.append(f"  â€¢ Bytes Sent:      {r['bytes_sent']:,} bytes")
        report.append(f"  â€¢ Bytes Received:  {r['bytes_received']:,} bytes")
        report.append(f"  â€¢ Overhead/Msg:    {r['overhead_per_msg']:.0f} bytes")
        report.append("")
    
    # Performance analysis
    report.append("â”€"*70)
    report.append(" ğŸ¯ PERFORMANCE ANALYSIS")
    report.append("â”€"*70 + "\n")
    
    avg_all = mean([r['avg_latency'] for r in results])
    throughput_peak = max([r['throughput'] for r in results])
    
    report.append(f"Overall Average Latency: {avg_all:.2f}ms")
    report.append(f"Peak Throughput:         {throughput_peak:.0f} req/s")
    report.append(f"Protocol Overhead:       ~{results[0]['overhead_per_msg']:.0f} bytes/message")
    report.append("")
    
    # Comparison to REST
    rest_latency = 100  # Typical REST latency
    rest_overhead = 500  # Typical REST overhead
    
    improvement_latency = rest_latency / avg_all
    improvement_overhead = rest_overhead / results[0]['overhead_per_msg']
    
    report.append("ğŸ“Š Comparison to REST API:")
    report.append(f"  â€¢ Latency:  {improvement_latency:.1f}x FASTER")
    report.append(f"  â€¢ Overhead: {improvement_overhead:.1f}x SMALLER")
    report.append(f"  â€¢ Bandwidth Savings: {((rest_overhead - results[0]['overhead_per_msg']) / rest_overhead * 100):.1f}%")
    report.append("")
    
    # Verdict
    if avg_all < 10:
        verdict = "ğŸ† EXCELLENT - Sub-10ms latency achieved!"
    elif avg_all < 50:
        verdict = "âœ… GOOD - Well within acceptable range"
    else:
        verdict = "âš ï¸  NEEDS OPTIMIZATION - Consider caching/optimization"
    
    report.append(f"Verdict: {verdict}")
    report.append("")
    report.append("="*70 + "\n")
    
    return "\n".join(report)

async def main():
    print("ğŸš€ Starting Queztl Protocol stress test...")
    
    try:
        tester = QuickTest()
        results = await tester.run_all_tests()
        
        # Generate report
        report = generate_text_report(results)
        
        # Save to file
        with open("queztl_performance_report.txt", "w") as f:
            f.write(report)
        
        # Print to console
        print(report)
        
        print("âœ… Report saved to: queztl_performance_report.txt")
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        print("Make sure Queztl Protocol server is running on port 9999")

if __name__ == "__main__":
    asyncio.run(main())
