# ğŸš€ QUEZTL-CORE v1.2 - DISTRIBUTED AUTO-SCALING NETWORK

## ğŸ¯ What's New in v1.2

**v1.2 brings MASSIVE scalability** - from single-node to **100+ dynamically scaled distributed nodes**!

### Key Features

#### 1. **Distributed Multi-Node Architecture** ğŸŒ
- Master/Worker node topology
- Automatic node discovery and registration
- Heterogeneous compute support (CPU, GPU, ANE, TPU)
- Cross-platform (Linux, macOS, Windows, VMs, Docker)

#### 2. **Dynamic Auto-Scaling** âš¡
- **Reactive Scaling**: Responds to current load
- **Predictive Scaling**: ML-based demand forecasting
- **Cost-Optimized**: Minimizes cloud costs with spot instances
- **Geographic Distribution**: Places nodes near workload sources
- Scales from **1 to 100+ nodes automatically**

#### 3. **Real-World Benchmarks** ğŸ“Š
- **LLM Inference**: Test GPT/LLaMA-style models (tokens/sec)
- **Image Processing**: OpenCV-style filters and transforms
- **Video Encoding**: H.264/H.265 encoding (realtime factor)
- **Database Operations**: TPC-H style queries
- **Crypto Mining**: SHA-256 hash rates (MH/s)
- **Scientific Computing**: LINPACK-style GFLOPS
- **Web Server Load**: Request/sec with latency percentiles
- **ML Training**: PyTorch/TensorFlow operations

#### 4. **Cloud Provider Integration** â˜ï¸
- AWS EC2 (with spot instances)
- Google Cloud Platform
- Microsoft Azure
- Docker containers (local/remote)
- Bare metal servers

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MASTER NODE (Coordinator)                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   FastAPI    â”‚  â”‚  Auto-Scaler  â”‚  â”‚  Task Queue    â”‚  â”‚
â”‚  â”‚   Server     â”‚â”€â”€â”‚  (ML-based)   â”‚â”€â”€â”‚  & Scheduler   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚           Node Registry (Discovery)                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚           â”‚           â”‚           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WORKER NODE  â”‚ â”‚ WORKER   â”‚ â”‚ WORKER  â”‚ â”‚  WORKER  â”‚
â”‚   (CPU)      â”‚ â”‚  (GPU)   â”‚ â”‚  (ANE)  â”‚ â”‚  (Cloud) â”‚
â”‚              â”‚ â”‚          â”‚ â”‚         â”‚ â”‚          â”‚
â”‚ 4 cores      â”‚ â”‚ RTX 4090 â”‚ â”‚ M2 Max  â”‚ â”‚ AWS EC2  â”‚
â”‚ 8GB RAM      â”‚ â”‚ 24GB     â”‚ â”‚ ANE     â”‚ â”‚ t3.large â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚               â”‚            â”‚            â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
              Executes distributed
              benchmarks & workloads
```

---

## ğŸš€ Quick Start

### Option 1: Local Docker (Fastest)

```bash
# 1. Start master node
cd /Users/xavasena/hive
docker-compose up -d

# 2. Auto-scaler will launch worker containers automatically
# Watch it scale:
curl http://localhost:8000/api/v1.2/network/status

# 3. Submit a distributed workload
curl -X POST http://localhost:8000/api/v1.2/workload/submit \
  -H "Content-Type: application/json" \
  -d '{
    "workload_type": "llm_inference",
    "payload": {"model_size": "7B"},
    "priority": 8
  }'
```

### Option 2: Multi-VM Setup

**On Master Node:**
```bash
# Start coordinator
python -m backend.main

# API runs on http://master-ip:8000
```

**On Each Worker Node:**
```bash
# Install dependencies
pip install -r backend/requirements.txt

# Start worker (auto-registers with master)
python -c "
from distributed_network import WorkerNode
import asyncio

worker = WorkerNode(
    coordinator_url='http://master-ip:8000',
    port=8001
)

asyncio.run(worker.start())
"
```

### Option 3: Cloud (AWS/GCP/Azure)

```bash
# Configure cloud credentials
export AWS_ACCESS_KEY_ID="your-key"
export AWS_SECRET_ACCESS_KEY="your-secret"

# Start master with cloud scaling enabled
python -m backend.main --scale-provider=aws --max-nodes=100

# Auto-scaler will launch EC2 instances as needed
```

---

## ğŸ“Š Real-World Benchmarks

### Run Complete Suite

```bash
# Via API
curl http://localhost:8000/api/v1.2/benchmarks/realworld

# Via Python
python -c "
import asyncio
from real_world_benchmarks import RealWorldBenchmarkSuite

results = asyncio.run(RealWorldBenchmarkSuite.run_all())
RealWorldBenchmarkSuite.print_results(results)
"
```

### Example Output

```
================================================================================
ğŸ“Š REAL-WORLD BENCHMARK RESULTS
================================================================================

ğŸ”¹ LLM Inference (7B)
   Score: 156.23 tokens/sec
   Time: 12.34s
   Comparison:
      NVIDIA A100: 7.8%
      RTX 4090: 19.5%
      M2 Ultra: 39.1%
      Xeon CPU: 312.5%

ğŸ”¹ Video Encoding (H264 4K)
   Score: 42.5 fps
   Time: 7.06s
   Realtime Factor: 1.42x (âœ… Can stream realtime!)
   Comparison:
      NVIDIA NVENC (4090): 17.7%
      AMD VCE (7900 XTX): 21.3%
      Apple VideoToolbox (M2): 23.6%
      x264 Software (i9): 94.4%

ğŸ”¹ Database Operations (TPC-H Style)
   Score: 3,245 queries/sec
   Time: 0.92s
   Comparison:
      AWS RDS (r6g.16xlarge): 21.6%
      PostgreSQL (32 cores): 40.6%
      MySQL (16 cores): 64.9%
      SQLite: 649.0%

... [7 more benchmarks]
================================================================================
```

---

## âš¡ Auto-Scaling Configuration

### Scaling Policies

```python
from autoscaler import AutoScaler, ScalingPolicy, ScalingTarget

# 1. REACTIVE (responds to current load)
scaler = AutoScaler(
    registry=registry,
    scheduler=scheduler,
    policy=ScalingPolicy.REACTIVE,
    target=ScalingTarget(
        min_nodes=2,
        max_nodes=50,
        target_cpu_utilization=0.70,
        scale_up_threshold=0.85,
        scale_down_threshold=0.30
    )
)

# 2. PREDICTIVE (ML forecasting)
scaler = AutoScaler(
    policy=ScalingPolicy.PREDICTIVE,  # Uses historical patterns
    target=ScalingTarget(
        min_nodes=5,
        max_nodes=100,
        cooldown_seconds=60.0  # Fast scaling
    )
)

# 3. COST-OPTIMIZED (minimize cloud costs)
scaler = AutoScaler(
    policy=ScalingPolicy.COST_OPTIMIZED,  # Uses spot instances
    target=ScalingTarget(
        min_nodes=1,
        max_nodes=200
    )
)
```

### Manual Scaling

```bash
# Scale up by 5 nodes
curl -X POST http://localhost:8000/api/v1.2/scale/manual \
  -H "Content-Type: application/json" \
  -d '{"action": "up", "count": 5}'

# Scale down by 3 nodes
curl -X POST http://localhost:8000/api/v1.2/scale/manual \
  -H "Content-Type: application/json" \
  -d '{"action": "down", "count": 3}'
```

---

## ğŸŒ Geographic Distribution

Deploy nodes across multiple regions for low latency:

```python
from autoscaler import GeoDistributedScaler, GeographicRegion

scaler = GeoDistributedScaler(
    registry=registry,
    scheduler=scheduler,
    policy=ScalingPolicy.PREDICTIVE
)

# Nodes automatically deployed to optimal regions
# based on workload source locations
```

Supported regions:
- ğŸ‡ºğŸ‡¸ US East (Virginia)
- ğŸ‡ºğŸ‡¸ US West (California)
- ğŸ‡ªğŸ‡º EU West (Ireland)
- ğŸ‡¯ğŸ‡µ Asia Pacific (Tokyo)
- ğŸ‡¸ğŸ‡¬ Asia Pacific (Singapore)
- ğŸ‡¦ğŸ‡º Australia (Sydney)

---

## ğŸ“¡ API Endpoints (v1.2)

### Network Status
```bash
GET /api/v1.2/network/status
```

Returns:
```json
{
  "coordinator": {
    "node_id": "abc123",
    "hostname": "master-node",
    "uptime": 3600.5
  },
  "registry": {
    "total_nodes": 15,
    "online_nodes": 14,
    "available_nodes": 12,
    "total_cpu_cores": 96,
    "total_ram_gb": 256,
    "total_gpu_vram_gb": 120
  },
  "scheduler": {
    "pending_tasks": 23,
    "active_tasks": 45,
    "completed_tasks": 1567,
    "success_rate": 0.98
  },
  "nodes": [...]
}
```

### Submit Distributed Workload
```bash
POST /api/v1.2/workload/submit
```

Body:
```json
{
  "workload_type": "llm_inference",
  "payload": {
    "model_size": "7B",
    "prompt": "Hello world"
  },
  "priority": 8
}
```

### Auto-Scaler Status
```bash
GET /api/v1.2/autoscaler/status
```

Returns:
```json
{
  "policy": "predictive",
  "target": {
    "min_nodes": 1,
    "max_nodes": 100,
    "target_cpu": 0.7
  },
  "managed_instances": 23,
  "scale_up_count": 45,
  "scale_down_count": 12,
  "last_action": 120.5
}
```

---

## ğŸ® Use Cases

### 1. AI/ML Training
```python
# Distribute LLM training across 50 GPUs
task_id = await coordinator.submit_workload(
    workload_type=WorkloadType.NEURAL_TRAINING,
    payload={
        "model": "llama-7b",
        "dataset": "openwebtext",
        "epochs": 3
    },
    priority=10
)
```

### 2. Video Processing Pipeline
```python
# Encode 1000 videos in parallel
for video in videos:
    await coordinator.submit_workload(
        workload_type=WorkloadType.VIDEO_ENCODE_H264,
        payload={"input": video, "resolution": "4K"},
        priority=7
    )
```

### 3. Scientific Simulation
```python
# Run Monte Carlo simulation on 100 nodes
await coordinator.submit_workload(
    workload_type=WorkloadType.MONTE_CARLO,
    payload={"iterations": 10_000_000, "samples": 1_000_000},
    priority=9
)
```

### 4. Real-time Analytics
```python
# Process streaming data across edge nodes
await coordinator.submit_workload(
    workload_type=WorkloadType.STREAM_PROCESSING,
    payload={"source": "kafka://...", "window_size": 60},
    priority=10
)
```

---

## ğŸ”§ Configuration

### Environment Variables

```bash
# Master node
export QUEZTL_MODE=master
export QUEZTL_PORT=8000
export QUEZTL_MAX_NODES=100

# Worker node
export QUEZTL_MODE=worker
export QUEZTL_COORDINATOR_URL=http://master:8000
export QUEZTL_PORT=8001

# Auto-scaling
export QUEZTL_SCALE_POLICY=predictive
export QUEZTL_SCALE_MIN=1
export QUEZTL_SCALE_MAX=100
export QUEZTL_SCALE_COOLDOWN=60

# Cloud providers
export AWS_REGION=us-east-1
export GCP_PROJECT=my-project
export AZURE_SUBSCRIPTION=my-sub
```

---

## ğŸ“ˆ Performance

### Scaling Speed
- **Local Docker**: 2-5 seconds per node
- **AWS EC2**: 30-60 seconds per node
- **GCP Compute**: 25-50 seconds per node

### Throughput
- **Single Node**: ~1,000 tasks/hour
- **10 Nodes**: ~10,000 tasks/hour
- **100 Nodes**: ~100,000 tasks/hour

### Cost (AWS Spot Instances)
- **t3.medium** (2 vCPU, 4GB): $0.0104/hour
- **c5.2xlarge** (8 vCPU, 16GB): $0.085/hour
- **g4dn.xlarge** (4 vCPU, 16GB, 1 GPU): $0.156/hour

**Example**: Running 50 t3.medium nodes for 1 hour = $0.52

---

## ğŸ› Troubleshooting

### Nodes Not Registering
```bash
# Check network connectivity
ping master-node

# Check master is running
curl http://master-node:8000/api/health

# Check worker logs
docker logs queztl-worker-1
```

### Auto-Scaler Not Scaling
```bash
# Check scaler status
curl http://localhost:8000/api/v1.2/autoscaler/status

# Check if in cooldown period
# Check if at min/max node limits
```

### High Task Failure Rate
```bash
# Check node health
curl http://localhost:8000/api/v1.2/network/status

# Review failed tasks
# Check resource constraints (CPU, memory)
```

---

## ğŸš€ Next Steps

1. **Run Local Test**:
   ```bash
   ./start.sh
   curl http://localhost:8000/api/v1.2/network/status
   ```

2. **Add Worker Nodes**:
   - Spin up VMs or Docker containers
   - Point them to master URL
   - Watch auto-discovery happen

3. **Submit Workloads**:
   - Use API to submit real-world benchmarks
   - Monitor execution across nodes

4. **Enable Cloud Scaling**:
   - Configure AWS/GCP/Azure credentials
   - Set max_nodes to 100+
   - Let auto-scaler manage fleet

---

## ğŸ“š Related Documentation

- [ARCHITECTURE.md](./ARCHITECTURE.md) - System architecture
- [API_CONNECTION_GUIDE.md](./API_CONNECTION_GUIDE.md) - API integration
- [DEPLOYMENT.md](./DEPLOYMENT.md) - Deployment guide
- [3DMARK_COMPLETE.md](./3DMARK_COMPLETE.md) - WebGL benchmarks

---

## ğŸ¯ Roadmap

- [ ] Kubernetes orchestration
- [ ] WebAssembly worker nodes
- [ ] Apple Silicon ANE optimization
- [ ] TPU support (Google Coral, Edge TPU)
- [ ] FPGA integration
- [ ] Blockchain-based work distribution
- [ ] P2P node discovery
- [ ] Zero-trust security model

---

**v1.2 transforms Queztl-Core from a single-node system to a massively scalable distributed compute network!** ğŸš€
